{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import evaluate, make, utils\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    #need input space and output space\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=num_states, out_features=24)   \n",
    "        self.fc2 = nn.Linear(in_features=24, out_features=32)\n",
    "        self.out = nn.Linear(in_features=32, out_features=num_actions) # so num actions is gonna be wrong at some point cause you can't play in a certain position sometimes\n",
    "        \n",
    "    # t is input tensor\n",
    "    def forward(self, t):\n",
    "        # t = t.flatten(start_dim=1)\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experience class\n",
    "# create instances of experience objects that get stored in, and sampled from, the reply memory\n",
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0 # keeps track of how many experiences we have added to memory\n",
    "      \n",
    "    # push method stores experiences in replay mem\n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "            \n",
    "        # if memory full, we overwrite the old memories\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "    \n",
    "    # returns a random sample of experiences\n",
    "    # remember we need this to train the network\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    # can we sample from memory yet? ie we only have 2 experiences but our batch size is 50\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "        \n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * math.exp(-1. * current_step * self.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "        \n",
    "    def select_action(self, state, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "\n",
    "        \n",
    "        \n",
    "        legal_moves = [c for c in range(self.num_actions) if state.board[c] == 0]\n",
    "        \n",
    "        # explore\n",
    "        if rate > random.random():\n",
    "            # action = random.randrange(self.num_actions)\n",
    "            action = random.choice(legal_moves)\n",
    "            return torch.tensor(action).to(self.device) # explore, before had as torch.tensor([action]).to(self.device)\n",
    "        \n",
    "        #exploit\n",
    "        else:\n",
    "            # doing the torch.no_grad() before we pass data to the policy net\n",
    "            # to turn off gradient tracking b/c we using model for inference, not training\n",
    "            \n",
    "            state = torch.tensor(state.board).float().to(self.device)\n",
    "            with torch.no_grad():\n",
    "#                 return policy_net(state).argmax(dim=1).to(self.device) # exploit\n",
    "                #action = policy_net(state).argmax().to() # exploit\n",
    "    \n",
    "    \n",
    "                estimatedqvals = policy_net(state).to(self.device) # just using policy net output\n",
    "        \n",
    "                # gotta make sure this doesn't pick an already full column, so beef up the legal column q-vals\n",
    "                for move in legal_moves:\n",
    "                    estimatedqvals[move] += 100\n",
    "\n",
    "\n",
    "                return estimatedqvals.argmax().to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectX():\n",
    "    def __init__(self):\n",
    "#         self.device = device\n",
    "        self.env = make('connectx', debug=True)\n",
    "        self.trainer = self.env.train([None, \"random\"])\n",
    "        \n",
    "        \n",
    "#         self.env.reset()\n",
    "#         self.done = False\n",
    "\n",
    "        # Define required gym fields (examples):\n",
    "        config = self.env.configuration\n",
    "        self.action_space = gym.spaces.Discrete(config.columns)\n",
    "        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        return self.trainer.step(action)\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        return self.trainer.reset()\n",
    "\n",
    "    def render(self):\n",
    "        return self.env.render()\n",
    "        #return self.env.render(**kwargs)\n",
    "\n",
    " \n",
    "        \n",
    "    # other methods\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "    # should be like 7 or something, its the number of actions you can take\n",
    "    def num_actions_available(self):\n",
    "        return self.action_space.n # not just self.env.action_space.n cause env has none i guess\n",
    "    \n",
    "    def num_states(self):\n",
    "        return self.observation_space.n # or just self.observation_space.n??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this worked because the duration is exactly equal to the reward it gets \n",
    "def plot(values, moving_avg_period):\n",
    "    plt.figure(1)\n",
    "    plt.clf()        \n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('reward')\n",
    "    plt.plot(values)\n",
    "\n",
    "    moving_avg = get_moving_average(moving_avg_period, values)\n",
    "    plt.plot(moving_avg)    \n",
    "    plt.pause(0.001)\n",
    "    print(\"Episode\", len(values), \"\\n\", moving_avg_period, \"episode moving avg:\", moving_avg[-1])\n",
    "    \n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "def get_moving_average(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) >= period:\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1).mean(dim=1).flatten(start_dim=0)\n",
    "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
    "        return moving_avg.numpy()\n",
    "    else:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "        return moving_avg.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor processing\n",
    "def extract_tensors(experiences):\n",
    "    \n",
    "    # Convert batch of Experiences to Experience of batches, see below\n",
    "    batch = Experience(*zip(*experiences))\n",
    "\n",
    "    t1 = torch.stack(batch.state)\n",
    "    t2 = torch.stack(batch.action)\n",
    "    t3 = torch.stack(batch.reward)\n",
    "    t4 = torch.stack(batch.next_state)\n",
    "\n",
    "    return (t1,t2,t3,t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q value class - calculates the Q-vals\n",
    "\n",
    "class QValues():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     device = torch.device(\"cpu\")\n",
    "    \n",
    "    \n",
    "    # static methods - can call the methods without creating an instance of the class first\n",
    "    \n",
    "    # states and actions are the ones that are sampled from replay mem. returns predicted q-vals from policy net for those stat-action pairs\n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        states = states.to(device)\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
    "\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    remember for each next state, we want to obtain the maximum q value we could get for\n",
    "    that next state (across all the possible actions).\n",
    "    For a DQN, that max q value will be predicted by the target net\n",
    "    \n",
    "    The final states: when an episode ends (in our case its when the screen is all black, ie its all 0)\n",
    "        We want to know where these final states are (if any) b/c we don't want to pass them to target net to\n",
    "        get a predicted q-val. Q-val for final states is 0 b/c agent won't recieve a reward once episode has ended\n",
    "    \n",
    "    '''\n",
    "    @staticmethod\n",
    "    \n",
    "    # so hmmm I guess i'm not worried about selecting a final state here, or am i? so going to change this\n",
    "    # all we really need is the right sied of bellman (given the next state, what is highest q value of all possible next options)\n",
    "    def get_next(target_net, next_states):\n",
    "        next_states = next_states.to(device)\n",
    "        return target_net(next_states).max(dim=1)[0]\n",
    "        \n",
    "        # ok so here could easily write something to find if the next state is a final state (look if you fuck up and choose an extra col or you win). Or maybe is this just trying to filter out picking an already filled column?\n",
    "        # either way i think its just trying to make it so we don't select a final state as a best possible next optino. but why not? aren't some final states good?\n",
    "        # anf also aren't i gonna filter out stupid moves anyway?\n",
    "        \n",
    "        # so here they are finding anything where its all black (if the max of all the inpupts is 0, then its all 0s -> all black)\n",
    "        \n",
    "#         final_state_locations = next_states.flatten(start_dim=1).max(dim=1)[0].eq(0).type(torch.bool)\n",
    "        \n",
    "#         non_final_state_locations = (final_state_locations == False)# just an exact opposite of the final_state_locations\n",
    "#         non_final_states = next_states[non_final_state_locations] # getting the value of those states\n",
    "        \n",
    "#         batch_size = next_states.shape[0]\n",
    "#         values = torch.zeros(batch_size).to(QValues.device) # initializing tensor and sending to device\n",
    "        \n",
    "        # returns tensor with 0s for q-vals for final states, and target net's maximum predicted q-vals across all actions for each final state\n",
    "#         values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune these hyperparameters to experiment with\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "# batch_size = 32\n",
    "gamma = 0.99\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.001\n",
    "target_update = 10\n",
    "memory_size = 100000\n",
    "lr = 0.001\n",
    "num_episodes = 400\n",
    "\n",
    "# mydevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mydevice = torch.device(\"cpu\")\n",
    "\n",
    "env = ConnectX()\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "\n",
    "agent = Agent(strategy, env.num_actions_available(), mydevice)\n",
    "memory = ReplayMemory(memory_size)\n",
    "\n",
    "policy_net = DQN(env.num_states(), env.num_actions_available()).to(mydevice)\n",
    "target_net = DQN(env.num_states(), env.num_actions_available()).to(mydevice)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict()) # setting the weights and biases to be the same as the policy net\n",
    "target_net.eval() # tells pytorch that network is not in training mode, only used for inference\n",
    "# lets try without these 2 things later\n",
    "\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# episode_durations = []\n",
    "# env.reset()\n",
    "# state = env.reset()\n",
    "# # state = torch.tensor(state.board).float()\n",
    "# # print(state)\n",
    "\n",
    "# nactions = env.num_actions_available()\n",
    "\n",
    "\n",
    "# legalmoves = [c for c in range(nactions) if state.board[c] == 0]\n",
    "# action = random.choice(legalmoves)\n",
    "# action = torch.tensor(action).to(mydevice) # explore\n",
    "# print(action)\n",
    "\n",
    "\n",
    "# state = torch.tensor(state.board).float()\n",
    "# with torch.no_grad():\n",
    "# #   return policy_net(state).argmax(dim=1).to(self.device) # exploit\n",
    "#     estimatedqvals = policy_net(state) # exploit\n",
    "#     print(estimatedqvals)\n",
    "#     for move in legalmoves:\n",
    "#         estimatedqvals[move] += 100\n",
    "#     print(estimatedqvals)\n",
    "    \n",
    "#     action = estimatedqvals.argmax().to(mydevice)\n",
    "#     print(action)\n",
    "\n",
    "\n",
    "# # \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_list = [256, 512]\n",
    "lr_list = [.01, .001, .0001]\n",
    "gamma_list = [0.99, 0.999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training loop\n",
    "\n",
    "episode_durations = []\n",
    "episode_rewards = []\n",
    "\n",
    "# implementation of algorithim\n",
    "for episode in range(num_episodes):\n",
    "    env.reset()\n",
    "#     state = env.get_state() #why need?\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    r = 0\n",
    "\n",
    "    for timestep in count():\n",
    "        action = agent.select_action(state, policy_net) # is this expecting state as a tensor...\n",
    "\n",
    "        next_state, reward, done, info = env.step(action.item()) # item returns value of tensor, since action is a tensor, before had # obs, reward, done, _ = env.take_action(action) #this take_action is same as step, but just wrapps the reward in a tensor # reward = env.take_action(action)\n",
    "        # env.render()\n",
    "\n",
    "        # fix rewards\n",
    "        if reward == 0:\n",
    "            reward = -1\n",
    "        elif reward == 0.5:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = 1\n",
    "        r += reward\n",
    "\n",
    "        # yeah so right now reward is just going to be 0 or 1, and i should just do it at end of episode. like no need to keep storing it\n",
    "        # but then how will it train on a good move or not? can only evaluate at end...\n",
    "        # cause most of the rewards in the experience buffer are just 0, so not very helpful for training...\n",
    "\n",
    "\n",
    "\n",
    "#        memory.push(Experience(state, action, next_state, reward_tensor))\n",
    "        memory.push(Experience(torch.tensor(state.board).float(), action, torch.tensor(next_state.board).float(), torch.tensor(reward, dtype=torch.float, device=mydevice))) # but why the [reward] and not just reward???\n",
    "        state = next_state\n",
    "\n",
    "\n",
    "        if memory.can_provide_sample(batch_size):\n",
    "            experiences = memory.sample(batch_size)\n",
    "            states, actions, rewards, next_states = extract_tensors(experiences)\n",
    "\n",
    "            current_q_values = QValues.get_current(policy_net, states, actions) # returns q values for any state action pairs as predicted by the policy net, returned as a pytorch tensor\n",
    "\n",
    "            # for each next state, we want to obtain the maximum q value (predicted by target net) among all possible next actions\n",
    "            # This function will return the maximum q-values for the next states using using the best corresponding next actions\n",
    "            next_q_values = QValues.get_next(target_net, next_states)\n",
    "            target_q_values = (next_q_values * gamma) + rewards # calculating the target q-vals using the formula\n",
    "\n",
    "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1)) # calc loss using mean squared error\n",
    "            optimizer.zero_grad() # if we didn't zero out gradients, we would be accumulating gradients across all back prop runs.?????\n",
    "            loss.backward() # computes gradient of loss wrt all weights and biases in the policy net\n",
    "            optimizer.step() # updates the wrights and biases with the gradients that were computed when we called backward() \n",
    "\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(timestep)\n",
    "            episode_rewards.append(r)\n",
    "#             plot(episode_rewards, 100)\n",
    "\n",
    "            # ok so these episode_durations are just basically the reward it got... since its directly the episode duration. \n",
    "            # let see what the other guy is plotting\n",
    "\n",
    "            break\n",
    "\n",
    "\n",
    "    # check to see if we need to update the target network (remember we set it to multiples of 10)\n",
    "    if episode % target_update == 0: \n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invalid Action: Invalid column: 3\n",
    "Invalid Action: Invalid column: 3\n",
    "gamma: 0.99 lr: 0.01 batch_size: 32\n",
    "My Agent vs Random Agent: 1.0\n",
    "My Agent vs Negamax Agent: 0.0\n",
    "\n",
    "gamma: 0.999 lr: 0.01 batch_size: 32\n",
    "My Agent vs Random Agent: 0.6\n",
    "My Agent vs Negamax Agent: 0.0\n",
    "\n",
    "gamma: 0.99 lr: 0.001 batch_size: 32\n",
    "My Agent vs Random Agent: 0.8\n",
    "My Agent vs Negamax Agent: 0.0\n",
    "\n",
    "gamma: 0.999 lr: 0.001 batch_size: 32\n",
    "My Agent vs Random Agent: 0.7\n",
    "My Agent vs Negamax Agent: 0.0\n",
    "\n",
    "gamma: 0.99 lr: 0.0001 batch_size: 32\n",
    "My Agent vs Random Agent: 0.6\n",
    "My Agent vs Negamax Agent: 0.0\n",
    "\n",
    "gamma: 0.999 lr: 0.0001 batch_size: 32\n",
    "My Agent vs Random Agent: 0.9\n",
    "My Agent vs Negamax Agent: 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "400 episodes cpu: \n",
    "- Wall time: 1min 34s\n",
    "\n",
    "400 episodes gpu:\n",
    "- Wall time: 1min 54s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiences_test = memory.sample(50)\n",
    "# print(experiences_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate your Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an agent to work with kaggle evaluate() method\n",
    "def my_agent(observation, configuration):\n",
    "\n",
    "    # get set of legal moves\n",
    "    legal_moves = [c for c in range(configuration.columns) if observation.board[c] == 0]\n",
    "\n",
    "    # get the current layout of the board (why float tho, need to be floats for my dqn to work?)\n",
    "    board = torch.tensor(observation.board).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        estimatedqvals = policy_net(board)\n",
    "\n",
    "        # gotta make sure this doesn't pick an already full column, so beef up the legal column q-vals\n",
    "        for move in legal_moves:\n",
    "            estimatedqvals[move] += 100\n",
    "\n",
    "#         return int(estimatedqvals.argmax().to(device)\n",
    "        a = estimatedqvals.argmax().item()\n",
    "#         print(f\"selecting {a}.\")\n",
    "        return estimatedqvals.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Agent vs Random Agent: 0.9\n",
      "My Agent vs Negamax Agent: 0.0\n",
      "Random Agent vs. My Agent: 0.2\n",
      "Negamax Agent vs. My Agent: 1.0\n"
     ]
    }
   ],
   "source": [
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)\n",
    "\n",
    "# Run multiple episodes to estimate its performance.\n",
    "print(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\n",
    "print(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))\n",
    "print(\"Random Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"random\", my_agent], num_episodes=10)))\n",
    "print(\"Negamax Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", my_agent], num_episodes=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-2b66fd261ee5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'c' is not defined"
     ]
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_tensors(experiences):\n",
    "    \n",
    "#     # Convert batch of Experiences to Experience of batches, see below\n",
    "#     batch = Experience(*zip(*experiences))\n",
    "\n",
    "#     t1 = torch.cat(batch.state)\n",
    "#     t2 = torch.cat(batch.action)\n",
    "#     t3 = torch.cat(batch.reward)\n",
    "#     t4 = torch.cat(batch.next_state)\n",
    "\n",
    "#     return (t1,t2,t3,t4)\n",
    "\n",
    "# next line is current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "#     def get_current(policy_net, states, actions):\n",
    "#         return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1)) \n",
    "#     '''\n",
    "#     remember for each next state, we want to obtain the maximum q value we could get for\n",
    "#     that next state (across all the possible actions).\n",
    "#     For a DQN, that max q value will be predicted by the target net\n",
    "#     '''\n",
    "    \n",
    "\n",
    "\n",
    "# states, actions, rewards, next_states = extract_tensors(experiences)\n",
    "# current_q_values = QValues.get_current(policy_net, states, actions) # returns q values for any state action pairs as predicted by the policy net, returned as a pytorch tensor\n",
    "\n",
    "batch_test = Experience(*zip(*experiences_test))\n",
    "print(batch_test)\n",
    "\n",
    "# states_test1 = torch.cat(batch_test.state)\n",
    "# print(states_test1)\n",
    "\n",
    "states_test2 = torch.stack(batch_test.state)\n",
    "print(states_test2)\n",
    "\n",
    "actions_test = torch.stack(batch_test.action)\n",
    "print(actions_test)\n",
    "\n",
    "next_states_test = torch.stack(batch_test.next_state)\n",
    "print(next_states_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QValues.get_current(policy_net, states_test2, actions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = next_states_test.shape[0]\n",
    "values = torch.zeros(batch_size).to(mydevice) # initializing tensor and sending to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_net(next_states_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_net(next_states_test).max(dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# returns tensor with 0s for q-vals for final states, and target net's maximum predicted q-vals across all actions for each final state\n",
    "values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
    "return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qvals_test = torch.tensor([[1.0143],\n",
    "        [1.0678]])\n",
    "\n",
    "next_q_vals_test = torch.tensor([0.0631, 0.0623, 0.0639, 0.0626, 0.0624, 0.0627, 0.0624, 0.0625, 0.0623,\n",
    "        0.0623, 0.0623, 0.0623, 0.0627, 0.0633, 0.0623, 0.0621, 0.0622, 0.0625,\n",
    "        0.0624, 0.0623, 0.0628, 0.0000, 0.0625, 0.0620, 0.0622, 0.0622, 0.0624,\n",
    "        0.0623, 0.0623, 0.0621, 0.0623, 0.0623, 0.0000, 0.0623, 0.0622, 0.0626,\n",
    "        0.0628, 0.0623, 0.0000, 0.0626, 0.0624, 0.0622, 0.0623, 0.0626, 0.0626,\n",
    "        0.0624, 0.0624, 0.0625, 0.0000, 0.0624, 0.0624, 0.0633, 0.0626, 0.0622,\n",
    "        0.0623, 0.0624, 0.0624, 0.0627, 0.0627, 0.0623, 0.0623, 0.0623, 0.0631,\n",
    "        0.0624, 0.0627, 0.0633, 0.0623, 0.0625, 0.0624, 0.0623, 0.0625, 0.0626,\n",
    "        0.0623, 0.0624, 0.0623, 0.0626, 0.0628, 0.0622, 0.0623, 0.0624, 0.0625,\n",
    "        0.0623, 0.0626, 0.0623, 0.0623, 0.0623, 0.0625, 0.0624, 0.0626, 0.0625,\n",
    "        0.0631, 0.0628, 0.0619, 0.0623, 0.0624, 0.0623, 0.0624, 0.0623, 0.0623,\n",
    "        0.0632, 0.0626, 0.0623, 0.0000, 0.0629, 0.0623, 0.0629, 0.0624, 0.0623,\n",
    "        0.0624, 0.0623, 0.0624, 0.0624, 0.0624, 0.0625, 0.0625, 0.0623, 0.0623,\n",
    "        0.0624, 0.0628, 0.0623, 0.0000, 0.0626, 0.0623, 0.0622, 0.0623, 0.0623,\n",
    "        0.0625, 0.0626, 0.0624, 0.0623, 0.0633, 0.0624, 0.0623, 0.0620, 0.0625,\n",
    "        0.0623, 0.0624, 0.0626, 0.0643, 0.0624, 0.0623, 0.0624, 0.0623, 0.0630,\n",
    "        0.0625, 0.0626, 0.0624, 0.0625, 0.0624, 0.0622, 0.0623, 0.0624, 0.0625,\n",
    "        0.0628, 0.0634, 0.0623, 0.0622, 0.0624, 0.0618, 0.0625, 0.0625, 0.0622,\n",
    "        0.0623, 0.0627, 0.0622, 0.0622, 0.0624, 0.0643, 0.0623, 0.0623, 0.0626,\n",
    "        0.0624, 0.0631, 0.0624, 0.0625, 0.0627, 0.0625, 0.0625, 0.0625, 0.0624,\n",
    "        0.0624, 0.0000, 0.0625, 0.0621, 0.0623, 0.0625, 0.0624, 0.0626, 0.0623,\n",
    "        0.0624, 0.0623, 0.0624, 0.0000, 0.0623, 0.0624, 0.0618, 0.0623, 0.0639,\n",
    "        0.0623, 0.0626, 0.0000, 0.0621, 0.0639, 0.0624, 0.0623, 0.0623, 0.0623,\n",
    "        0.0630, 0.0645, 0.0624, 0.0627, 0.0622, 0.0621, 0.0633, 0.0629, 0.0000,\n",
    "        0.0623, 0.0623, 0.0626, 0.0626, 0.0627, 0.0624, 0.0625, 0.0625, 0.0628,\n",
    "        0.0630, 0.0624, 0.0621, 0.0622, 0.0623, 0.0629, 0.0625, 0.0626, 0.0623,\n",
    "        0.0624, 0.0623, 0.0624, 0.0000, 0.0631, 0.0624, 0.0622, 0.0623, 0.0624,\n",
    "        0.0636, 0.0624, 0.0631, 0.0623, 0.0623, 0.0625, 0.0624, 0.0623, 0.0627,\n",
    "        0.0623, 0.0624, 0.0000, 0.0624])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qvals_test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_q_vals_test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is kaggles connect4 env\n",
    "\n",
    "\n",
    "\n",
    "from kaggle_environments import make\n",
    "import gym\n",
    "\n",
    "class ConnectX(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.env = make(\"connectx\", debug=True)\n",
    "        self.trainer = self.env.train([None, \"random\"])\n",
    "        \n",
    "        # Define required gym fields (examples):\n",
    "        config = self.env.configuration\n",
    "        self.action_space = gym.spaces.Discrete(config.columns)\n",
    "        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.trainer.step(action)\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.trainer.reset()\n",
    "    \n",
    "    def render(self, **kwargs):\n",
    "        return self.env.render(**kwargs)\n",
    "        \n",
    "    \n",
    "env = ConnectX()\n",
    "\n",
    "done = False\n",
    "obs = env.reset()\n",
    "while not done:\n",
    "    # Choose first available empty column as the action.\n",
    "    action = [i for i in range(len(obs.board)) if obs.board[i] == 0][0]\n",
    "    obs, reward, done, info = env.step(action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deeplizard, comments are what other guy has\n",
    "batch_size = 256 # 32\n",
    "gamma = 0.999 # 0.99\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.001\n",
    "target_update = 10 # guessing this is \"copy step\" = 25\n",
    "memory_size = 100000\n",
    "lr = 0.001 #.01\n",
    "num_episodes = 1000 # episodes = 16000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# additions from other guy\n",
    "copy_step = 25\n",
    "hidden_units = [100, 200, 200, 100]\n",
    "\n",
    "max_experiences = 10000\n",
    "min_experiences = 100\n",
    "\n",
    "# epsilon = 0.5        looks like these all correspond\n",
    "# decay = 0.9999\n",
    "# min_epsilon = 0.1\n",
    "\n",
    "\n",
    "precision = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# future ideas and for write up\n",
    "\n",
    "\n",
    "Most of the top agents were not deep learning agesnts, and with some (since connect4 is a solved game and only on connect 4 for right now) implemented a perfect solution. Still used this as an exercise to get more experience with deep learning and pytorch\n",
    "\n",
    "\n",
    "So I'm not expecting the agent to actually do very well, \n",
    "\n",
    "\n",
    "From a notebook on the leaderboard:\n",
    "COOL:\n",
    "3) Using the left-right symmetry of ConnectX, I implemented a dual Replay Memory, that stores each experience but also its mirror image, effectively doubling the data and teaching the agent about the game's inherent symmetry.\n",
    "\n",
    "\n",
    "- difficult to train since there are so many things happening in a single game, and it only gets its reward at the end of the game. How is it supposed to know at which part it did something good or at which part it was something bad?\n",
    "\n",
    "And against a random agent it seems to just like to go in the same spot. makes sense if you think aobut it cause if it just does same thing, then likely it will just get 4 in a row going straight up\n",
    "    - what is probabillity that 4 times doesn't go in same space? easy calculation\n",
    "    \n",
    "    \n",
    "testing around with lots of hyperparrameters\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
