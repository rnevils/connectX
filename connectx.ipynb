{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import evaluate, make, utils\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    #need input space and output space\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=num_states, out_features=24)   \n",
    "        self.fc2 = nn.Linear(in_features=24, out_features=32)\n",
    "        self.out = nn.Linear(in_features=32, out_features=num_actions) # so num actions is gonna be wrong at some point cause you can't play in a certain position sometimes\n",
    "        \n",
    "    # t is input tensor\n",
    "    def forward(self, t):\n",
    "        # t = t.flatten(start_dim=1)\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experience class\n",
    "# create instances of experience objects that get stored in, and sampled from, the reply memory\n",
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0 # keeps track of how many experiences we have added to memory\n",
    "      \n",
    "    # push method stores experiences in replay mem\n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "            \n",
    "        # if memory full, we overwrite the old memories\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "    \n",
    "    # returns a random sample of experiences\n",
    "    # remember we need this to train the network\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    # can we sample from memory yet? ie we only have 2 experiences but our batch size is 50\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "        \n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * math.exp(-1. * current_step * self.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "        \n",
    "    def select_action(self, state, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "\n",
    "        \n",
    "        \n",
    "        legal_moves = [c for c in range(self.num_actions) if state.board[c] == 0]\n",
    "        \n",
    "        # explore\n",
    "        if rate > random.random():\n",
    "            # action = random.randrange(self.num_actions)\n",
    "            action = random.choice(legal_moves)\n",
    "            return torch.tensor(action).to(self.device) # explore, before had as torch.tensor([action]).to(self.device)\n",
    "        \n",
    "        #exploit\n",
    "        else:\n",
    "            # doing the torch.no_grad() before we pass data to the policy net\n",
    "            # to turn off gradient tracking b/c we using model for inference, not training\n",
    "            \n",
    "            state = torch.tensor(state.board).float()\n",
    "            with torch.no_grad():\n",
    "#                 return policy_net(state).argmax(dim=1).to(self.device) # exploit\n",
    "                #action = policy_net(state).argmax().to() # exploit\n",
    "    \n",
    "    \n",
    "                estimatedqvals = policy_net(state) # just using policy net output\n",
    "        \n",
    "                # gotta make sure this doesn't pick an already full column, so beef up the legal column q-vals\n",
    "                for move in legal_moves:\n",
    "                    estimatedqvals[move] += 100\n",
    "\n",
    "\n",
    "                return estimatedqvals.argmax().to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectX():\n",
    "    def __init__(self):\n",
    "#         self.device = device\n",
    "        self.env = make('connectx', debug=True)\n",
    "        self.trainer = self.env.train([None, \"random\"])\n",
    "        \n",
    "        \n",
    "#         self.env.reset()\n",
    "#         self.done = False\n",
    "\n",
    "        # Define required gym fields (examples):\n",
    "        config = self.env.configuration\n",
    "        self.action_space = gym.spaces.Discrete(config.columns)\n",
    "        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        return self.trainer.step(action)\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        return self.trainer.reset()\n",
    "\n",
    "    def render(self):\n",
    "        return self.env.render()\n",
    "        #return self.env.render(**kwargs)\n",
    "\n",
    " \n",
    "        \n",
    "    # other methods\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "    # should be like 7 or something, its the number of actions you can take\n",
    "    def num_actions_available(self):\n",
    "        return self.action_space.n # not just self.env.action_space.n cause env has none i guess\n",
    "    \n",
    "    def num_states(self):\n",
    "        return self.observation_space.n # or just self.observation_space.n??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this worked because the duration is exactly equal to the reward it gets \n",
    "def plot(values, moving_avg_period):\n",
    "    plt.figure(1)\n",
    "    plt.clf()        \n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('reward')\n",
    "    plt.plot(values)\n",
    "\n",
    "    moving_avg = get_moving_average(moving_avg_period, values)\n",
    "    plt.plot(moving_avg)    \n",
    "    plt.pause(0.001)\n",
    "    print(\"Episode\", len(values), \"\\n\", moving_avg_period, \"episode moving avg:\", moving_avg[-1])\n",
    "    \n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "def get_moving_average(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) >= period:\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1).mean(dim=1).flatten(start_dim=0)\n",
    "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
    "        return moving_avg.numpy()\n",
    "    else:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "        return moving_avg.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor processing\n",
    "def extract_tensors(experiences):\n",
    "    \n",
    "    # Convert batch of Experiences to Experience of batches, see below\n",
    "    batch = Experience(*zip(*experiences))\n",
    "\n",
    "    t1 = torch.stack(batch.state)\n",
    "    t2 = torch.stack(batch.action)\n",
    "    t3 = torch.stack(batch.reward)\n",
    "    t4 = torch.stack(batch.next_state)\n",
    "\n",
    "    return (t1,t2,t3,t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q value class - calculates the Q-vals\n",
    "\n",
    "class QValues():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    \n",
    "    # static methods - can call the methods without creating an instance of the class first\n",
    "    \n",
    "    # states and actions are the ones that are sampled from replay mem. returns predicted q-vals from policy net for those stat-action pairs\n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
    "\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    remember for each next state, we want to obtain the maximum q value we could get for\n",
    "    that next state (across all the possible actions).\n",
    "    For a DQN, that max q value will be predicted by the target net\n",
    "    \n",
    "    The final states: when an episode ends (in our case its when the screen is all black, ie its all 0)\n",
    "        We want to know where these final states are (if any) b/c we don't want to pass them to target net to\n",
    "        get a predicted q-val. Q-val for final states is 0 b/c agent won't recieve a reward once episode has ended\n",
    "    \n",
    "    '''\n",
    "    @staticmethod\n",
    "    \n",
    "    # so hmmm I guess i'm not worried about selecting a final state here, or am i? so going to change this\n",
    "    # all we really need is the right sied of bellman (given the next state, what is highest q value of all possible next options)\n",
    "    def get_next(target_net, next_states):\n",
    "        return target_net(next_states).max(dim=1)[0]\n",
    "        \n",
    "        # ok so here could easily write something to find if the next state is a final state (look if you fuck up and choose an extra col or you win). Or maybe is this just trying to filter out picking an already filled column?\n",
    "        # either way i think its just trying to make it so we don't select a final state as a best possible next optino. but why not? aren't some final states good?\n",
    "        # anf also aren't i gonna filter out stupid moves anyway?\n",
    "        \n",
    "        # so here they are finding anything where its all black (if the max of all the inpupts is 0, then its all 0s -> all black)\n",
    "        \n",
    "#         final_state_locations = next_states.flatten(start_dim=1).max(dim=1)[0].eq(0).type(torch.bool)\n",
    "        \n",
    "#         non_final_state_locations = (final_state_locations == False)# just an exact opposite of the final_state_locations\n",
    "#         non_final_states = next_states[non_final_state_locations] # getting the value of those states\n",
    "        \n",
    "#         batch_size = next_states.shape[0]\n",
    "#         values = torch.zeros(batch_size).to(QValues.device) # initializing tensor and sending to device\n",
    "        \n",
    "        # returns tensor with 0s for q-vals for final states, and target net's maximum predicted q-vals across all actions for each final state\n",
    "#         values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune these hyperparameters to experiment with\n",
    "\n",
    "batch_size = 256\n",
    "gamma = 0.999\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.001\n",
    "target_update = 10\n",
    "memory_size = 100000\n",
    "lr = 0.001\n",
    "num_episodes = 1000\n",
    "\n",
    "mydevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = ConnectX()\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "\n",
    "agent = Agent(strategy, env.num_actions_available(), mydevice)\n",
    "memory = ReplayMemory(memory_size)\n",
    "\n",
    "policy_net = DQN(env.num_states(), env.num_actions_available()).to(mydevice)\n",
    "target_net = DQN(env.num_states(), env.num_actions_available()).to(mydevice)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict()) # setting the weights and biases to be the same as the policy net\n",
    "target_net.eval() # tells pytorch that network is not in training mode, only used for inference\n",
    "# lets try without these 2 things later\n",
    "\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n",
      "tensor([ 0.2051, -0.0528, -0.0435,  0.0954, -0.1155,  0.0948,  0.0638])\n",
      "tensor([100.2051,  99.9472,  99.9566, 100.0954,  99.8845, 100.0948, 100.0638])\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# testing block\n",
    "episode_durations = []\n",
    "env.reset()\n",
    "state = env.reset()\n",
    "# state = torch.tensor(state.board).float()\n",
    "# print(state)\n",
    "\n",
    "nactions = env.num_actions_available()\n",
    "\n",
    "\n",
    "legalmoves = [c for c in range(nactions) if state.board[c] == 0]\n",
    "action = random.choice(legalmoves)\n",
    "action = torch.tensor(action).to(mydevice) # explore\n",
    "print(action)\n",
    "\n",
    "\n",
    "state = torch.tensor(state.board).float()\n",
    "with torch.no_grad():\n",
    "#   return policy_net(state).argmax(dim=1).to(self.device) # exploit\n",
    "    estimatedqvals = policy_net(state) # exploit\n",
    "    print(estimatedqvals)\n",
    "    for move in legalmoves:\n",
    "        estimatedqvals[move] += 100\n",
    "    print(estimatedqvals)\n",
    "    \n",
    "    action = estimatedqvals.argmax().to(mydevice)\n",
    "    print(action)\n",
    "\n",
    "\n",
    "# \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a66a7721020d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mepisode_durations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# ok so these episode_durations are just basically the reward it got... since its directly the episode duration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-368053f7c016>\u001b[0m in \u001b[0;36mplot\u001b[0;34m(values, moving_avg_period)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmoving_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_moving_average\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoving_avg_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoving_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episode\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoving_avg_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"episode moving avg:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoving_avg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_ipython\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mpause\u001b[0;34m(interval)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mcanvas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mdraw_idle\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_idle_drawing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_idle_draw_cntx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1916\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_cursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mRendererAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1707\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1709\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2647\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2649\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m         \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m         ticklabelBoxes, ticklabelBoxes2 = self._get_tick_bboxes(ticks_to_draw,\n\u001b[1;32m   1205\u001b[0m                                                                 renderer)\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_update_ticks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0mmajor_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_majorticklocs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m         \u001b[0mmajor_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajor_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m         \u001b[0mmajor_ticks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_major_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajor_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_locs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajor_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtick\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajor_ticks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmajor_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmajor_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mget_major_ticks\u001b[0;34m(self, numticks)\u001b[0m\n\u001b[1;32m   1409\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajorTicks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnumticks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m             \u001b[0;31m# Update the new tick label properties from the old.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1411\u001b[0;31m             \u001b[0mtick\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1412\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajorTicks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgridline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gridOnMajor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick\u001b[0;34m(self, major)\u001b[0m\n\u001b[1;32m   2221\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2222\u001b[0m             \u001b[0mtick_kw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_minor_tick_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mYTick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmajor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtick_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, axes, loc, label, size, width, color, tickdir, pad, labelsize, labelcolor, zorder, gridOn, tick1On, tick2On, label1On, label2On, major, labelrotation, grid_color, grid_linestyle, grid_linewidth, grid_alpha, **kw)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_tickdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtickdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick1line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick1line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick2line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick2line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgridline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_gridline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick1line\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    581\u001b[0m                           \u001b[0mmarkersize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m                           \u001b[0mmarkeredgewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_width\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m                           zorder=self._zorder)\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_yaxis_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhich\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tick1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_artist_props\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, xdata, ydata, linewidth, linestyle, color, marker, markersize, markeredgewidth, markeredgecolor, markerfacecolor, markerfacecoloralt, fillstyle, antialiased, dash_capstyle, solid_capstyle, dash_joinstyle, solid_joinstyle, pickradius, drawstyle, markevery, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_markerfacecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkerfacecolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_markerfacecoloralt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkerfacecoloralt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_markeredgecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkeredgecolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_markeredgewidth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkeredgewidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mset_markerfacecoloralt\u001b[0;34m(self, fc)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_markerfacecoloralt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_markerfacecoloralt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36many\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36many\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   2315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m     \"\"\"\n\u001b[0;32m-> 2317\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_or\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'any'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEWCAYAAABIVsEJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO2de5hkZX3nv9+uy3QVF5lhBkRug0CieAmGkWhwjVFRyBogG42QNaKrD2Y3ZF2zSYA1XmLiE5LdDdls3EQSUTRGMBrDBDGKgBqjIIMgV5GrMpkRhjszXT116d/+cc7pOl196pz3ct5zqX4/z9NPd1WdqvN2VZ33976/25ciAo/H4/F4dJkrewAej8fjqSfegHg8Ho/HCG9APB6Px2OENyAej8fjMcIbEI/H4/EY4Q2Ix+PxeIzwBsTjcQjJBsndJI/I81iPpwrQ14F4PGNI7o7d7ALYC2AU3n6XiHy6+FF5PNXEGxCPZwokHwTwThH5asoxTREZFjcqj6c6eBeWx6MByT8keTnJz5B8BsBbSL6c5PUknyS5k+Sfk2yFxzdJCsnN4e2/DR//EslnSH6b5FG6x4aPn0ryBySfIvl/Sf4rybcV+4541jLegHg8+vwSgL8D8CwAlwMYAng3gI0ATgJwCoB3pTz/VwG8D8AGAD8C8Ae6x5I8CMBnAfxOeN4HAJxo+g95PCZ4A+Lx6PNNEfknEVkSkZ6I3CgiN4jIUETuB3AxgJ9Lef7nRGSbiAwAfBrA8QbHvgHALSJyRfjYRQAetf/XPB51mmUPwOOpIQ/Fb5B8HoD/DeAEBIH3JoAbUp7/49jfCwD2NTj2OfFxiIiQ3J45co8nR/wOxOPRZzLz5KMAbgdwjIjsD+D9AOh4DDsBHBbdIEkAhzo+p8ezAm9APB579gPwFIA9JJ+P9PhHXlwJ4KdJ/iLJJoIYzKYCzuvxLOMNiMdjz38HcDaAZxDsRi53fUIReRjAmwH8KYDHABwN4GYEdSsg+SqST0bHk3wfyX+K3f4Kyd91PU7PbOPrQDyeGYBkA8AOAG8UkX8pezyetYHfgXg8NYXkKSSfRXIdglTfIYDvlDwszxrCGxCPp768AsD9CNJ3TwFwhojsLXdInrWEd2F5PB6Pxwi/A/F4PB6PEWuqkHDjxo2yefPmsofh8Xg8teKmm256VERWpYmvKQOyefNmbNu2rexheDweT60g+cOk+70Ly+PxeDxGeAPi8Xg8HiO8AfF4PB6PEd6AeDwej8cIb0A8Ho/HY0SpBoTkJSQfIXn7lMcZSnreS/JWkj8de+xskveEP2cXN2qPx+PxAOXvQD6BoAXDNE4FcGz4cw6AvwQAkhsAfADAzyCQ8fwAyfVOR+rxeDyeFZRaByIi3yC5OeWQ0wF8UoJ+K9eTPIDkIQBeBeBqEXkcAEhejcAQfcbFOL9w83bs2TvCW152pPFrfOu+R3H9fY+lHnNAt423n7QZgTZQdbny1h046eiNWL9Pu+yhzAQ7nuzhrp1P4zXPP9j4NX7w8DO48ns7Uo9Z12rg115+JPafbxmdY+9whI//64NY2DtMPe41zz8YP3X4AUbnAICv3vkwbt3+ZPaBM8LLjj4QP3v0Rmevf/ePn8EXb92Bt/7sZmzcd12ur131QsJDsVI+dHt437T7V0HyHAS7FxxxxBFGg7jyezvx46cXrQzIh794F+7Y8TSm2YaoJdkrf2IjjjloP+PzuOaJPX2c+3c34wO/eBzeftJRZQ9nJvjU9T/Ex/7lAdz9h6cYLx4u/sb9+NxN2zO/X4dv6OK0n3qO0TluevAJXPil7wNA6nlu+7en8PG3n2h0DgB47z/ehoef3jv1HLOECHDd3bvwT7/5CmfnuGvn0/jza+/FL/30YWvOgCR9hSTl/tV3ilwM4GIA2LJli1HnyE67gV5/ZPLUZfbsHeK0n3oO/vyslyQ+fu33H8Z/+sQ27N5rdx7X7A5Xn7sX01ehHnV2Lw7RHy2hP1rCumbD6DX27B3iJw7eF195z88lPr7zqR5e/kfXYk/G7iF1nOFzr/zNV+CFhz4r8ZgzL/429lh+h/fsHeEdrzgK73vDcVavUwfO/bvv4s4dTzs9x0I4d3XbZt+tNMqOgWSxHcDhsduHIRDNmXa/E7rtxvKHYMpCf5T6AXZazfC4ak/M0fuwMKi2oasT0Xtqs0hZ6I/QaU9fD3aXv1/m5+gNsieibruJhYH5d1hEsNAfOpnsqkgec0sW0ZzSWYMGZCuAt4bZWC8D8JSI7ATwZQCvI7k+DJ6/LrzPCd1203pi7/VHmG+lGJDww7Xd6bgmeh+qPs460QsnXKvJvT9CpzX9cp5vz4XHmX+Po/GlTUSdlt2EuHe4hCVB6rUySwTvl9tFY3Stdhy8p6W6sEh+BkFAfCPJ7Qgyq1oAICJ/BeAqAL8A4F4ACwDeHj72OMk/AHBj+FIfigLqLphvNZZXXyaICBYG6TuQ6DHXqxFboi9j1XdKdWJ5V2ezAxkMcdB+81Mfbzfm0Jij3TkiV0hr+rRh6+7tOXS3VJFOu2k1t6iwMBih1SBajfz3C2VnYZ2V8bgA+I0pj10C4BIX45qk225gMBIMRktGH0J/tITRkmS4sOqyA7Gf7Dwryc+FNf37RRJdy91BT8EVYuuSWVBwk80StnOLCsHu1M37WXUXViWw3R0sbyHTfNTL56j2yj66wKtu6OpEHru6Xn+EbsYkYbs7WOiP0Jwj2s3p04b9DiQyUlXP78mHIjwPQUzJzfvpDYgCtvEJlSyI6AOuenA6usD9DiQ/IsNh89lnJWkA4e7A8hxZgdhuq4n+aAnD0ZLxOYLXWRs7kCJinyrfDVO8AVHAdnegYkDmW3Mgq7+y91lY+dPLwYXVy8jCAkJ/u+0uR8FIAebfD5cpp1WkCM9DT8Hwm+INiAIdyxRIlSwIktYZLEUw9tdX29VWJ6LJ1vSzH4Y1JEo7EMv4RJYrxHZFPXb3rg0DYju3qOB3ICUTvfmm2RLR6iLr4isiJ9yWsb++2uOsE7ZGWTXwbPv96vWHmcFYW5/+eAeytmIgLjOxFgbZu1NTvAFRwPqiGKitqoIAZLVX9nlkDHnGjJYE/WEQL7BP0sj4frXsg+jKLixjd2+02FobO5Aigui9/tBZTMkbEAXG23Kzi0I1t73bcp8Tbks0vqqPsy7EJ1r7VXv25G7zuakE0aOV7qLheXqKi61ZwQfR1wDLGVKOL/BOLVxYYSX6YAQRo9ZinhjxicPWRdpJKfADgsndttpdfQfig+gqRHNLz6L9SxY+iF4y1nUgiquqbg5NG10TvQciwOLALFXTMyZuNEw/+0WNGIhVFpZKEL2VT83UvGFTybpRTB2I34GUin1myQwF0WMTXtWLHutA/PMuwoW1YLFzVKoDsb1WBkHV9NzcGujlDvcurKUlCd5TH0QvjygAZe5iUGtmVkRfHFvymPA8YxZWuLDs6oxUkjREgoaFJqgEY+3dvWunEy8wnltcXUuLQ7cuQW9AFGg25tBuzFlty9c1g2Z2aXQL6Mxpy0IOPnvPmF4OBnmcpJGxw7WYrFQaggJjI2ZTdLtWAuiA/dySheuYkjcgitik2Kr6IOsSRN9v3n3x01ohmmj3mzcPcKu7sMw1Z/YOlyCS3aPK2oXl0F9fVVym77ts5Q54A6KMTXwiMCDZPsi6BNEjWcyq75bqQLSL27jvOotea2qCQTb+dlUj1WrModWgVSuTtdJIMcJl7HMsAuZjIKXSsWhE1xsMlbbl3XYDw1hhWRXp9Uc4cJ/28t8eO6KJ48B92sYGedmF5bBKXEfVzqZgUaWr8KxhM7dk4V1YFcFmd6Duwgpzwis6MUd+8AP3DQyId2HZs2xA9m2bf78GI7Qbc2hm6El0LAyIjtCTjYLnwmBtBdEBt54Hl3K2QMkGhOQpJO8meS/J8xMev4jkLeHPD0g+GXtsFHtsq+uxdlsWF0VfTdBl3Mm0mq6hSBhrwz6BC6uqhq5ORL7vDfusM9/h9keYT5GzjVgWLTP4fqlmEgJ2sbyF/gjza82AWMwtWbiOgZTmbCTZAPARACcD2A7gRpJbReTO6BgReU/s+N8E8JLYS/RE5PiixttpN/DkQt/oub3+CBvDVXsaVZe17cXcLYCPgeTBQn+Exhyxf8cmiK4mGGSTYquaKgx4F5YuNnNLFrPswjoRwL0icr+I9AFcBuD0lOPPAvCZQkaWgF0QXS0GMl9xWdu4uwXwmiB5sBBOmN1WE/1hsMMzeg3FGFt0vC7RrkU1GcQu4WSNGRCHMg6u2+OXaUAOBfBQ7Pb28L5VkDwSwFEAro3dPU9yG8nrSZ4x7SQkzwmP27Zr1y7jwdp8yIEmsdqFB1R3BxKNa323XQvxqzoQ9Smy6WKr2uuoiCys6Dw27jifhZUfqlISppRpQJKq6qYtv84E8DkRib/LR4jIFgC/CuDPSB6d9EQRuVhEtojIlk2bNhkPtmPRyVSlAAuovi56PJBaB/GrOhB9N2wnd9c7EJ0YiGnPLVVhrFnDZm7JQlUrxpQyDch2AIfHbh8GYMeUY8/EhPtKRHaEv+8H8DWsjI/kTrBKcFxI2Kp2FlZ8NVOHvl11oNcfohO+n4Dh5K7Y6yhqUGgyuetnYZn9H6rnmCVs5pYsev0RSGBd081UX6YBuRHAsSSPItlGYCRWZVOR/EkA6wF8O3bfepLrwr83AjgJwJ2Tz82TTruJxcESljR91JFgkGodCFBhF1asq3AdxK/qQLS4sIpPKAoGzc2ZyybrKAV2DNNS15qcbYTp3KJCFGMj3TSnLM2AiMgQwLkAvgzgLgCfFZE7SH6I5GmxQ88CcJmsbCH6fADbSH4PwHUALoxnb7nAVHpSR2FtnMZbTQMSX4UGqYfVHGediAxIx0IXQifwHHXk1aXXH4KEUrpw19pIrS0D4lLW1nVlf6nRKhG5CsBVE/e9f+L2BxOe9y0AL3I6uAniK8R91qm/beNVldrKLXhONVf28Qvcpd92LdHrj3DQfussdyDqDQhNdwdRLZPKSjZSPlxaEq227KrCWLOG6dyiQs9xd2Nfia5IxzDFdnnSVQo+RjGQarYyiQtj1aFvVx2IKq9thJh0dyBGBkQxEQQYL5Z028brxFlmieizN5UBTsN1WrQ3IIosF2Fpuhh0tuWNOaLdnKtsJXrPB9FzJ0pbNe1iqysY1Gk3DV1Y6rsc02zCtevCctfdOvhueANSOqYuhsinrXPxVXVlH0/lrIP4VR0YB9HNJhFdwaBuyyz5YaE/RFfRtWTac0un2n2WcJm+73cgFSH6Ui+aurAUV4imAcgiiAtj1UH8quqISKgz3ohNum5X7aY7Rx2hJ9OgsE61+yzhUtZ2QbGI2RRvQBQx3oFoXuCmQc4iiK9m6iB+VXXGIk3jNF5dP7huszzT79eiRgzE/FpZWvH8tYLL9H0fRK8Ipim28cCz2nncdea0JS6MVWVXW12IJ1gsCzFpu0g1d7gWOxDlRVDLTPnQdevxquIyfd+7sCrCWKvDrYuhyiv7uDBWHcSvqs5knyKTIj99F5bZAkWnR5VpQoCqMNasYTq3qKCT/GCCNyCKdA3TLMerTPWLr6rB6ZUurGq3XakDk5XX3XbTIE1cb9VuWr8TVTSrYOqSURXGmjVM5xYVehquRxPW1idlgWlmSU/zAq9yemxcGKvq4ld1YHL3YFIlrhtj67YaGIwEg5HezlFVkgAwDwq7Xi1XFRulyDT6wyUMl8RpUoI3IIqsa85hzqCF+UJ/hGZY36FCp6W/Ci2KXmwHUvW+XXVgMm3VpL+YiYs0/jxVdFay45Rk/RjIWgugA+ZzSxau1QgBb0CUIWnUZVQn/RFw25nTlrjynWllvmfMZNqqSQwkev/nNbKw4s9TYTBawmAk6plekUtGu2+cmvTzrGE6t2SxoFmDZoI3IBoE/mO9yT2+alehyi6suIvBZfXsWmFy92CSQKErGGRStKZb4DffmjMSHFurLizAbG7JoojKfm9ANDCZ3IMeQuo+yE67gb2G0qauifdDMi1884yZFGkySY3W1dAYp9iqn2ccZ1H7HpNmbePXopxthIuFo3dhVQwzF8NQ6wN02drZlriLwbuw7EnKwtJNStAVDDL5fulIEsTPY7LYWmtythEuFD51u2CY4A2IBiZVvLoxkHFX1mqt7CeFsXwQ3Z4kF5bJ90tHMMjkczPpUWWSEKAqjDWLuOhAUURhpjcgGpgEuHW35VWtr5hchVZd/KoORBNsJDVr0gdNVzDIRHOmp+kmA2AkOOZdWPkuGotoj1+qASF5Csm7Sd5L8vyEx99GchfJW8Kfd8YeO5vkPeHP2UWMt2NwUfQ0M0uqurKfFMaquvhVHYhcgpHoUlREulJ8Mx3dXkcmyQ8mwViTgsU1HUR3oPBZRBC9NIcjyQaAjwA4GcB2ADeS3JogTXu5iJw78dwNAD4AYAsAAXBT+NwnXI7ZpEo8EgxSxVVRkS2Twlg+C8ueSZGmTrsJEWBxsKQ8kequ2k0WKMvFsBpdXY1iIGt8B5J33HNBsw+fCWXuQE4EcK+I3C8ifQCXAThd8bmvB3C1iDweGo2rAZziaJzLmFwUOj2EgPEEXT0X1srVTCR+VbVx1onJFbdJiq2uYJBJHYjJSlb3WtEVxpo13GRhuW+PX6YBORTAQ7Hb28P7JvllkreS/BzJwzWfC5LnkNxGctuuXbusBmwc5DRwMVQtCyupq3CV+3bVgcnKa5Pdp/b3K1qgaGVhmbiwmlqt6XWFsWaNTruhrTWUxWSauAvKNCBJaSOTzt9/ArBZRF4M4KsALtV4bnCnyMUiskVEtmzatMl4sMA40KXqo44LBqlS1fqKpFqAKotf1YHJALhZiq2eYFCzMYd2Y86oDkSro4Km4NhalbONiPqg6cS/sogLwLmiTAOyHcDhsduHAdgRP0BEHhORveHNvwZwgupzXdBtN7EkQF+xEd3iYCwYpH6OqrqwVtcCVFn8qg70JjrcmsYndCdd3RRbk3oC3ar6Ioreqky33QxS5TWbXKZRREypTANyI4BjSR5Fsg3gTABb4weQPCR28zQAd4V/fxnA60iuJ7kewOvC+5yiWzy3POnOQhZWoguruuJXdWDyAjcRYjKZJHT97QuDIdqaK1ndqvoiit6qjIvC3LgAnCtK+7REZEjyXAQTfwPAJSJyB8kPAdgmIlsB/FeSpwEYAngcwNvC5z5O8g8QGCEA+JCIPO56zPHJ/YBu9vGmKzegejGQJBdDlcWv6sBkANxk92mS+tpp67WN1+3nBqwUHFPpRG1S7T5L6M4tKsQF4FxRqrkXkasAXDVx3/tjf18A4IIpz70EwCVOBziBbpBz0SCNrt0IVnpVW9knCWN12w08vqdf1pBqz+TEbBIDMREM0t0dTLraVIgXxKoYEF3p51nDRfr+rLuwakdXs0rcJDBIspLB6SRhrCp3Dq4D8fb4gP4kYioYFFSJa7jJNFOFAX3BsSKqpquM7tyiQhHt8b0B0UA3T9+kh1B0fNWC00nCWFUWv6oDq11YepOIaeBZ9/vVM/Cl68byfBZW/tmXJq5HXbwB0WB5hajoYpgUDFKliiv7pKaQVRa/qjqRSJNNFtbCwCxuoB1E15CzjdANCk+2yllr6M4tKvQ0pSRM8AZEA90gp+mqquNAncyWpNVMFQ1dXUjana5rRkJMjne4mi5S3X5u8TGp70CidilreweS546+iN5i3oBo0NUU4zGtBA0qvKu1sk8Sxqqy+FXVSSrM1BVisnJhaRYrmuxygucqGkODjr+zhO7cosKCphaRCd6AaKDbgdY0MFjFlX2SMFaVxa+qzrS01agiWe01zGondF2Pupo2wLimRceFpSOMNWu46G7ts7AqhnlgUO8C77SqGUSf/DJ22vqFb56Aae4nnQC3qWBQ0KdqCUuKO0fTVOFgjOrXio4w1qyRdwHxaEmwd6je1dkUb0A0GKsFqq6qhiCB+Zbe21zFHUhiEL2inYPrwDSRJp0UW5sdbnwMWUymG+ucQ2c3tVYD6ID+3JKFiQiYCd6AaDA3R8y35jQuvCD4qLuqqlMQHahe25U6MC3BQqe63zRJQ+dzW1qSQJ/EMIiu7u7V7+k1S+jOLVmMd6c+C6tS6PR/WhiYFfIELqxquYUWBqtjIPPegBizLGebEFdSdmEZVm/rpNiarmR1BcfWsphUREezg3Eay7tTH0SvFrpZMiY+SBetnW1JEsbyLixzpsXHdNyXpoJBy5O7Qqaf6S5HV3BMVxhrFunm6HkoqjDTGxBNdIOcJh9gp92ACLB3mF9rZ1uSVohdH0Q3ZroLq6nlIgXM0sTjz0/DpsBPxxj6HUi+HShMa4R08QZEE92LwvTCi55fBaYJY1W1c3AdmCbSpCPEZCoYFJ1TRQHPtNodCP4XvXjh2g2iA/kmzyTVGbnAGxBNdFJsTbqYAtWrr9g7TBbGqqr4VR0Ydze2C6IbTewaCxSblazOinqtB9GBcG7JOYjuXVgVI4hPqLeaMHNhRUVY1XANTZvsqrZTqhMLgyHajTk0GysvQa0gukGTw+gcwRjUXVhmCyGNhBPvwtJus59GUe3xSzUgJE8heTfJe0men/D4b5G8k+StJK8heWTssRHJW8KfrZPPdUW3rd6B1jQw2M05J9yWhSnBWu/CMmdagkVciCnzNQwFg3QWKDZKgTq7qSL6NlWdPBU+Zz6ITrIB4CMATgVwHICzSB43cdjNALaIyIsBfA7An8Qe64nI8eHPaYUMGsUE0au2sp/mr6+q+FUdmLbijgsxmb5GFjoLFNNqdyDq6ZZ9DhEJe62tbQPiIojedRxXKnMHciKAe0XkfhHpA7gMwOnxA0TkOhFZCG9eD+Cwgse4Ct1eRaYrN6A6sYVpq5mqil/VgbQdCKCeYmtUZ2SQhWW6EFI5R38UNORcq3roETpzSxZJAnAuKNOAHArgodjt7eF903gHgC/Fbs+T3EbyepJnTHsSyXPC47bt2rXLbsQoZluuW4TlmrRAahXFr+rAtN2pboqtycS+rjmHOarvcuLj0kFVcMy0q/CsoTO3ZJEkAOeCMk1+Uu5hYuUcybcA2ALg52J3HyEiO0g+F8C1JG8TkftWvaDIxQAuBoAtW7ZYV+Z1W030wxbmaemTy3KjFllYVXENpQljVbFvVx0Imgeufj91qsQDI9TVPjdJ5aI1m2Csatffta5GGKE6t6hg0kHZhDJ3INsBHB67fRiAHZMHkXwtgPcCOE1E9kb3i8iO8Pf9AL4G4CUuBxuhOrnbXHhVC06nXeBV7NtVBxanJFgsy9oqfPaLA/Nuq4EmSPbk3uuPgqryhv5Uobq4KCpjqOrkuXAsQs4WKNeA3AjgWJJHkWwDOBPAimwqki8B8FEExuOR2P3rSa4L/94I4CQAdxYxaNX4hE0hT9WC6GkVz1UUv6oD04PoegFu00lCdXK3abOuKjhWVNFb1ckz9pkkAOeC0j4xERmSPBfAlwE0AFwiIneQ/BCAbSKyFcD/BLAvgL8Pv8A/CjOung/goySXEBjBC0WkEAOiOrnbFPLMN6tlQNICqd12A7v3egOiyzQXw7g4010QHVDv6dYbDJebZpqcI3iNEfZdN32qMW3JMmvkuXDs9UerGnW6oFSTLyJXAbhq4r73x/5+7ZTnfQvAi9yOLhl1A2K+LZ+bY6U68qbVAnRaDex6Zu+q+z3pTBNpUtWFsBUMUk1+sCnwi7tk0g1IMRlDVSdXAzIoprLfV6Jrspynn+G2sRV0qVJwOk0Yq0rjrBPTRJpUJ5F8vl+OdzmKNS02qcKzhOrcokJRlf3egGiiuwMx/RCrlB6bJozlg+j6pIk0qfrBbQWDOi3FLKxcdiBur5VZIW8XVhEuQW9ANFF1MSwX8hhWglZpZZ9WJRz076mGq60upO0eVGuAbAWDVKvETeRsI1QTAkyFsWaNPGVt/Q6koqh2oLXfgTRzq0q1Ja0gsoriV1Un7bsRCTFlVaLbfr90srBMJ3ZVwTFTYaxZI8/u1kVpzKeegeSGtMdF5PF8h1N9VFeI1hd4pYLow6k9deLiV0VkfcwCWSJNKl1ZbQWDVF2k04L9KqgKjvksrIA8O1AU1R4/y0TdhKA6nACOAPBE+PcBAH4E4Cino6sg4215RhDd8gLvthv48dMDo+fmzUJ/NDWVM77t9gZEjWh3MW3CVOkvZls7EQXRRSS1xsPGFaJaEGsqjDVrqM4tWRTZnDLVhSUiR4nIcxHUavyiiGwUkQMBvAHAPzgfXQVRbTVhu6qar1AQPU0Yq2ptV+pA1u5UZXdgKxjUbTexpCCbHARjzY0UoLZbX+sBdECvjU0a0wTgXKAaA3lpWLMBABCRL2FlX6o1Q7s5h+YcM+MT0wSDVNGRA3VNmhsjcsMsVmSsdSBrd6qiC2Hb/iOarNI+NxGxrnYH1BZbaz3+AYznFtvrfpoAnAtUZ7dHSf4eyc0kjyT5XgCPuRxYlVFZIdoK5FQpCys1iF4x8as6oLIDcR5jU9gd7B0uYcliJavswjIUxppF8ujIO00AzgWqBuQsAJsAfCH82RTetyZRDXLabMs7GsqHrkn7X6rWt6sOZLmfVFJsbQWDVFJsbQv8VAXHvAtrTB6ytrbxVx0yv32hcuAFIvJu56OpCV2FFNs8diD90RKGoyVjN1hepNUCVE38qg6oZGFtf0KxzsgyQyrtc1uwrHZXFRyzqXafNVTmliyKLMzMnJlEZATgBOcjqREqfapsfMdAXJmu/Ik5Tdu9auJXdSDLR60ixGQrGKSS/NCzrHYPnqvm7vU7kIA8euDZpnjroPrNuJnkVgB/D2BPdKeIrMlMLJX4xDTBIFXiK/v951vGr2PLYLSEwWi6MJbPwtInKwCu0qfKVjCoo7BAySMYq3atmAljzSJ5xD7TBODyRvUMGxAEzV8du0+wVlN52w08s5idJbO+2zY+R1ViC1mrmaqJX9WBhf4QcwykZZNQmURsV+0qGVJ5uEJUeqXZuntnCZW5JYsiXVhKBkRE3u56IHWi227gkafTW5gv9Ec49ACLC6+lVsXrmqyCtaoYujoRpa1OK+CLCzFNK66zFQzqtrJdj3kEY1UEx4oqeqsDKnNLFkVW9it9A0nOA3gHgKnBuVgAACAASURBVBcAmI/uF5H/5GhclSYIdGVXotteeNHrlElWxlDVxK/qQNZ3oxvb1U3T0ej1h1YTxNhFOv17nKYDo4qK4FhR+t11QGVuyaLI9viqEbhPAXg2gNcD+DoC/fJnbE9O8hSSd5O8l+T5CY+vI3l5+PgNJDfHHrsgvP9ukq+3HYsOqpXCuQTRSzcg6avQqolf1YGs/lIdhR5StqmvKt8v22p3IAoKTz/HaEnQHy5ZxQtniTxkHPIw/KqoGpBjROR9APaIyKUA/j0sFQHD9OCPADgVwHEAziJ53MRh7wDwhIgcA+AiAH8cPvc4BBrqLwBwCoD/F75eISj1KrJ0MehoY7tERbioSkWPdSArbXW5Srw/vc1IWmacCvMKBaBRlbpNj7OswrhxQoFvDA6ozS1ZRIu5aTG2PKFKG26S3xGRE0l+A8B/AfBjAN8J+2SZnZh8OYAPisjrw9sXAICI/FHsmC+Hx3ybZDM87yYA58ePjR+Xds4tW7bItm3b9Af7pfOBH9+2fPOhJxbwb0/28DNHbQCx2kctENzwwOM49IAODl9vll3SG4zwve1P4uhN+2LTvuuMXiMPnuz18f0fP4MXPGd/7LcuORvs5oeewH7zLRyzad+CR1dP7tr5NEYieOFznpX4+GN79uKeR3bjxYc+a+oi5NbtT2Jdq4GfPHg/43Hc8OBjePb+8zhywz6Jj+94qocfPb6Al27egEZKw8U07n90N57YM8AJR65PfLw/WsJ3f/QENh+4D569/3ziMWuJrLlFhR8+tgcPP7MXJ26ONVN/9ouAUy80HhfJm0Rky+T9qibqYpLrAbwPwFYAdyLcDVhwKICHYre3h/clHiMiQwBPAThQ8bkAAJLnkNxGctuuXbsshxwwF15MS1Nsb3S/TXfR6LlLJetsLIX/zFzKBDJHLh/nyWYkkjohz4Wf/Sjlsw9ew24cjYzPbbT82ZufY45M/T+i7/da78QbkTW3qDASsfrMtBCRUn4AvAnA38Ru/xqA/ztxzB0ADovdvg+BAfkIgLfE7v8YgF/OOucJJ5wgefCJf31AjjzvSnn0mcXExx95elGOPO9K+eS3HjA+x1O9vhx53pXy19+4z/g18uDzNz0kR553pTywa/fUY077i2/Kr33shgJHVW9ef9HX5Z2X3jj18evve1SOPO9K+eY9u6Yec8IfXC3nf/5Wq3H87B9dI++5/Oapj3/4i3fKT/7eVVbn+J///H3ZfP6VsrS0lPj4nTuekiPPu1KuunWH1Xlmhay5RYX3XHaznHThNTmOSgTANkmYU5V2ICTvI/lpkr+eEKcwZTuAw2O3DwOwY9oxoQvrWQAeV3yuM7LiE1mtKlSoSpNClZzyKolf1YGsILpKdX8egkFZAVsbOdv4OSSlbXyRVdN1II/YZ5G9xVRdWMcB+CiC1f//Ink/yS9YnvtGAMeSPIpkG0FQfOvEMVsBnB3+/UYA14bWcCuAM8MsraMAHAvgO5bjUSaeZpnEwsA+e6XZmEO7MVe6AVGpBfBBdD2yLvAsYSHJSTAo63PLo0dVVraXrTDWrJE1t6iwMChGzhZQr0QfARiEv5cAPAzgEZsTi8iQ5LkIxKoaAC4RkTtIfgjBdmkrAtfUp0jei2DncWb43DtIfhZBLGYI4Dck6NlVCFkXRV6rqmCFWO7KXiUlMI/Uw7VElkhTVg1QXoJBWSm2efSoire62bDP6s4MeaQKzxJ5pO/3+sNCtEAAdQPyNIDbAPwpgL8WkVy0QCQQqbpq4r73x/5eRBArSXruhwF8OI9x6JJVJb68qsph9Vb2yn5hMEQ7Q260CuOsC6Ig0qS6QMnj+7Vr9/Sq5zxcIZ2Mrr+2wlizRh4dKBb6Izx7/2L65+nogUQpvJeR/H2Sr3E3rGqTtULMq5Cn026U3o1XZRWqoqDnCVARacrqL5aXYFA3Q3Mmjx5VkZGb/r8UVzVdB/LoQFFkbzHVXlhXALiC5PMQFP79NwC/C6DjcGyVJXuFaKfVED/PYgViIFl+8PlWA4uDdG1tT8ByTCnlPc0SYspLMCgziD4Y4qD97Goz1HdTPgYC5BQDqVoQneTnSd4H4P8A2AfAWwEkVwatAeYzVlW5XeA5VKXasqBQ8RwXv/KkoyLSlCXElNeqvZuxw82jR1WW4JitMNasodIhIIs8sudUUT3LhQC+W2Sgusoou7AsfdSddhNP9QZWr2GLmgtrrC2xf8nqiVVHdcJM2x3kmaSR1Y3XPs6SnpJsK4w1a+TiwrJsc6OD6qd2B4ALSF4MACSPJfkGd8OqNlkXRV6BwSrUVyz0h5nuBS9rq45qfCwtMSEvwaBuq4l+2DY+iTxcIVmCY74T70psFT6zBODyRtWAfBxAH8DPhre3A/hDJyOqAfOtOZDTW2FnCQapUoXsJpWAXFU6B9cBVfdTmhBTni6s4PWmx1ps6wmyEgK8nO1KsuaWLIouzFSd4Y4WkT9BUAsCEekBhp2+ZgCSqfGJLMEgVapQX6GyCq2K+FUdUI2PpQkx5SUYlLZzHI6W0B8t5WikphXd2nWtnjWy5pYsii7MVDUgfZIdBDK2IHk0ADvZrJqTFoDMK42uCjsQFRdDVcSv6oDq7iHVhZX7DmT1eVSC/SpkCY7ZCmPNIlnJDWkUXZiZaaYYLKP/CsA/Azic5KcBnATgbW6HVm2ygpx5fICddhO9wQhLS7LcobVosvo2Ad6FpcPyBZ4VV2o1sOuZ5DVaXnVGaZ9bXpmEWYJjRaac1gUbz0PRLqzMb6CICMl3A3gdgJchcF29W0QedT24KtNtTS+ey6OHEDC+wBeH5W3zVVICqyJ+VQdUEyzSdyDB926+ZRdjW64ST3CV5Vngl/a/LPRH2G/eu7DipM0tWagIwOWJ6id3PYDnisgXXQ6mTqSlQPYG9p1SgZUrxDIMyNKSYHGwlGkMuykTkWcleQXRO62GfYwtpeZguRg2h4XQfErPrV5/hIP2K08wrYpkpVenUXRlv+qs9PMA3kXyhwD2INiFiIi82NnIKk43w4W1Tw4TfnTxlhVbUF3NeBeWOqoB8FS3Tw6deAFVF5b99zh1B5LTYmuWSJtbshjvTqtlQE51Oooa0m038ORCcpFfrz/CxhxkaG1zwm1RXy37ILoqi4MR5ltzmTGtKJAqIqt2GnklaaR9brm7sFITTrwLK07a3JJFXvExVVR7Yf3Q9UDqRhTgTkIl8KxCVp6+a1RXoVURv6oDqm0m4kJMk6vJvGonUncgA7WdkgppsgS+DmQ1aXNLFkW7sHz/AEOCXkVuM0vKXtmrCmNVRfyqDqgmWKSlRuclGNRNqd/JK1U4eI3keE5ewlizRtrcksViwe3xvQExJDWI3k8XDFKl7NiCTkpgFcSv6oDqijveX2z1a+QjGKTmwsohljfFp5+XMNaskUsQvWKtTHKF5AaSV5O8J/y9qrMvyeNJfpvkHSRvJfnm2GOfIPkAyVvCn+OL/Q+mB7pUBIN0zgEkTyJFoCOMVYWixzqgujsdCzElp9jm8f1qN+fQnGPi9ysvSQIAUzsLFz3Z1QWbIPpCf4R2Yw7NgpqalrUDOR/ANSJyLIBrwtuTLAB4q4i8AMApAP6M5AGxx39HRI4Pf25xP+SVdNsNDJcE/eHKFuYqgkGqpE0iRaCzCq2C+FUdUA2Ap8WV8hQMmrY7yNeFleySyUsYa9aYNreo0OsPC93RlWVATgdwafj3pQDOmDxARH4gIveEf+9AoMG+qbARZjBNqjPXC6/k4LTOKtRm1bSWCNJWsyfM1DYjOQaep07ugxFaDaKVw0p2WlA4r2r3WSNLBjiNoiv7yzIgB4vITgAIfx+UdjDJEwG0AdwXu/vDoWvrIpJTc2ZJnkNyG8ltu3btymPsAOLupZUXX149hIDyK7x1jKFN9exaQrV9eXp8Ij/BoG67iV6CmqSKEqX6ORoYjASDCcExL2ebzLS5RQUVAbg8cWZASH6V5O0JP6drvs4hAD4F4O0iEn0DLwDwPAAvBbABwHnTni8iF4vIFhHZsmlTfhuYaVkyY8Eg+wt8XXMOcxxnVhSNTluETruROBF5VqIq0pRWA5SnYNC0gsV8jVRyS/ei+zbVBZvmpEWnRTtzPorIa6c9RvJhkoeIyM7QQDwy5bj9AXwRwO+JyPWx194Z/rmX5McB/HaOQ1diWhuIPAODJKemQBaBzgXebTew86me6yHVHlUXw7QaoLwFg6YlP+TpConvpvafby3fn5cw1qyR1mImCxUBuDwpy4W1FcDZ4d9nA7hi8gCSbQBfAPBJEfn7iccOCX8TQfzkdqejTWDc/8ntqmq+RF30Xn+EOQJtBT94FfTb60CvP8K8jgvL8fdrWsponoH6afEc78JKJkuEK408PzcVyjIgFwI4meQ9AE4Ob4PkFpJ/Ex7zKwBeCeBtCem6nyZ5G4DbAGxECeqInXbw1k1eFHkHBrsl1lfoCGNVQfyq6iyLNCmsEKdNunkLBk1Lfsh1BzKlYDEvYaxZw6b+q+ggeil7RxF5DMBrEu7fBuCd4d9/C+Bvpzz/1U4HqEB0UUxO7nmvqsqsr+gN1FMCfR1INjoJFtOEmPIWDOq2m4nB2oXBCM/qtBKeYXKOafFCvwNJYtrcokLRGvO+Et2Q6dtyNcEgVYLgdHkxENWLOy5+5UlGZ3c6TYjJhQsruQ4kn2p3QMWF5WMgcWx2IHn14VPFGxBDproYcu5FU+bKXkcYKy5+5UlGd3ea9NnnLRiUViWedxB9tTtuCNJeGGvWsHNh5Zc9p4L/5AyZlqeftwur0yovC0snJbDsvl11QNf9lLQ7cOEi7YVt4+PkG0RPFhzLSxhr1jBtoqoqAJcn3oAYMi1PP+/AYLlBdPXVTNniV3VAV6QpcQeyrBSYl4u0CRFgcbC6yC9PIxW95opz+E68iZjqABUtZwt4A2JMY45oN+dWBSB7/aGSYJAqpbuwlHcg5Ypf1QHd3UOntbq/WO4xkFaUTTj+Hi8tSVCsmNMiaH7K4qLXHxWmnFcnps0tWZRRmOkNiAVJKZB565eXmR6rE5ArW/yqDujuTpNa5Ofvwlpt+KM4Vl5KgWkJJ34HkoxJb7lezt4PFbwBsSApAJlnDyFgpbRp0ehlYXkXVhY9RYGuiKQuBHnXGSUVreVtpFqNObQaTHT3ejnbZKYlN6SxUEJlvzcgFkwLcua5quq2mxgtCfqj4vtM6Qhj+SB6Nrppq6lBdIcpti665CalJKv2BVuLmHgeyqjs9wbEgmCFOHFR5BwYLCs4rSuMVbb4VR3QnZi7rdU1QL1BvoJBnQTXo4tgbOJuygfRp5I0t2RRRnt8b0AsSOojlHcvmrJW9rrCWGWLX9UB3crraVlY+X6/VmtPuFjJRq7YOEX3baoTJrK2ZVT2ewNiQZRDH0dVMEiVsjRBtCe7ksWv6oCuSFOn3SzARbr6c1vIOVUYKMbdO0skzS1Z5KlFpIo3IBYkrRDz7kWTtEIsAt0vY9niV3VAN8Gi226gP1rCMBb/ylswKMlF6mIlm6R8WHTVdJ0wSd/PU4tIFW9ALOi0Vq8Q8w4MlpUeq/tljMSvfBbWdHQnzKS4Ut6CQUnfLxcurKTdVJ7CWLNG0tySRd4JFip4A2JBogsr5wvcRhvAhl4/WPWqfhkj8auyGj/WAd3vRlJqdN6CQeM2I+Ndjotg7GRCQN7CWLOGkQvLB9HrRdK2vJdzbruNvKUNJm3DTQJ/awndoPG0FNs8J4j51hzIlckP488+3+/xyjhL8ZNdnUiaW7KIBODWNYub1ksxICQ3kLya5D3h7/VTjhvFxKS2xu4/iuQN4fMvD9ULC6fTbmBxsLTcwnxZMCjXlVs5LUIWDLoKl9m3qw5o70AShJjy3uGSXKUm6SIYOxlEz1sYa9aYnFtU0BGAy4uydiDnA7hGRI4FcE14O4meiBwf/pwWu/+PAVwUPv8JAO9wO9xkuhPuJVcXXvy1i8LkAveytukEAXD9GMhkim3eq/bJFNtefwTmvJJdvQPJVxhr1picW1TQEYDLi7IMyOkALg3/vhSBrrkSoQ76qwF8zuT5edKZ6CPkxHe8PIkUu7I3CaSa+G3XEroiTYkuLAfFd5O7g4UwESTPleyk4Jh3YaUzObeoUEZadFkG5GAR2QkA4e+Dphw3T3IbyetJRkbiQABPikg0o24HcOi0E5E8J3yNbbt27cpr/ABWp0C60HieL6m+IjJYOt1SfQwkHd3dQ1JqtIvU126rucpNlncq6KTgWBmtx+uESQcKHQG4vHDmgCT5VQDPTnjovRovc4SI7CD5XADXkrwNwNMJx011FIrIxQAuBoAtW7bk2pFwnGYZXHwutuWNOWJdc66EILr+Bd5pNfH4np6rIdUe/SD6SiEmV4JBk4a/56BLbnw3FW9r4g1IMpNziwp5p3ir4MyAiMhrpz1G8mGSh4jITpKHAHhkymvsCH/fT/JrAF4C4PMADiDZDHchhwHYkfs/oMDkClFXMEiVMjRBTHZTPoieTuQaUmXSheVq1T7ZOtyFK2RyRZ23MNasYVKYW0ZhZlkurK0Azg7/PhvAFZMHkFxPcl3490YAJwG4U4K+5tcBeGPa84ugO8WFlf8FXrysbW8w0hbGKlP8qupEIk02dSDuvl+NVXGW/AP1K336fgeSzuTcooKLBIssyjIgFwI4meQ9AE4Ob4PkFpJ/Ex7zfADbSH4PgcG4UETuDB87D8BvkbwXQUzkY4WOPmTaReHCxTCpJ+0ak9VMmeJXVcdEpKnTKmaH25koAHWxA5msePcGJB0Thc8yuhuXsn8UkccAvCbh/m0A3hn+/S0AL5ry/PsBnOhyjCpMtsLWFQxSpSwXlq4hjItfFZmLXgdMJsxJIabIH577AqU1tyqIvr7byvUck7K20e95b0ASSWqzn0UZQXRfiW7BZJ6+rmCQKmXUV5gE5MoUv6o6pinecSGmolykQct4N1lYq1xYvpVJIiYdKMpoj+8NiAWrgpyOcttN9JFtMXFjlCV+VQdMJ//45O7q+zWtDiRPJhtDLgyGuQpjzRq6OkC6AnB54T89CyYbHbpdIRbdjVd/NeNlbadjmuIdrxJ39v1qNTBcEvSHwc7RxUp2nBAwdHaOWUK3iWokALdWsrBmgnZjDo05rggM6ggGqVJGcNpEGMtrgkxnefegmbYa/+xdtf+IZ3uJCBYcBGOTEk58AH06k3NLFuPvl9+B1AaS6MbiE73+0MkHmCQH6hqTlMCyxK/qgGkNR7wrq7s6o3ByHwzRHy1htCROCwkBvwPJYnJuyaIMNULAGxBr4ivEII0u/y1kGS1CTISxyhK/qgOm7qcgxTZ0LUWThKv4RH+ExVAHJm8jta4ZtY2PXyvegKSh43lwFR/LwhsQS+Iptq625d1WE/1hsDIsCqMgekmdg+uA6QXeTcjCchefGC2nCuf9PZ5cUectjDWL6KTvl9Ue3xsQSzoTWTIuVgBlrOxNhLHKEr+qA6YiTfFJxJVgULx1uMsCv3jBondhZdPR6EBRVnt8b0As6bYbWBy43YEULWtrKowVrSi9AVmNqY96ZRDdjWBQ3IXlMhgb75Xmg+jZxOeWLEwE4PLAGxBL4kFOXcEgnXMAxU3MpgFf78KajqlI04odiCPBoM6y4R86K4YNXnOlu9fvQNLRkbXtOdw5puENiCWdiSwsF5W1RddXGPvrJ3L9PWNMRZriQkzOYmyx71c0YTkxVO2x4JgPomej04FiXNlfbAzER7EsiavwuXNhFauLbpwxVJL4VR0wFWmKCzG56nUUNyCdlruV7ModSPGtx+uGjsJnz6HhT8PvQCwpMohelAtr3FVY7wKfmyPmW8WLX9UBU5GmyfiE0xhb33EQvRVcK66EsWYNvSB6OS4svwSwJN6nytkOZHllX4xryKarcBnaJXXA9LsR7y/matUerxKfb7sLxkZBdC9nq4ZODzxXUhJZ+B2IJVGgKxIMchpELyg4bbOaKaNzcB0wFWmKT+6uAs+NOaLdnMPCYLjsCnEZRPdaIGpEc0ugoZeOiQBcHngDYsl8q4ElAZ5eHABwswIousdUdJ55g/+lDPGrOmAav4jXALkMPEerXZcr2Sgl2VVLllkjmlv2DrPlEcqKKZViQEhuIHk1yXvC3+sTjvl5krfEfhZJnhE+9gmSD8QeO774/yIguqAf3d1fcTvXc7SKDaLbpAR6WdtkjF1YE/EJZwYk3Dn2+iOsawaN/HI/R9jTzVW1+6yhE/ssQ0wKKG8Hcj6Aa0TkWADXhLdXICLXicjxInI8gFcDWADwldghvxM9LiK3FDLqBKIP+bHdewG4S38EikuPtakF8C6sZExFmiaD6Ca7QhXmYzsQV5k8nVYDoyXBUwvuduuzxKSGShquEiyyKMuAnA7g0vDvSwGckXH8GwF8SUQWnI7KgGhSeGyPux1IuzmH5hwLdGGZpwSWIX5VB0xFmpYTKAZREN2dC2shLCR0pRI4ea34QsJ0ovdLZeFYVmFmWQbkYBHZCQDh74Myjj8TwGcm7vswyVtJXkRy3bQnkjyH5DaS23bt2mU36gSiiy3agbi6wIvsyGvnwipe/KoOmKZ4R895aqHvVDCoG6bYuqp2B1bv1r0LK52uRl1Vb9ZcWCS/SvL2hJ/TNV/nEAAvAvDl2N0XAHgegJcC2ADgvGnPF5GLRWSLiGzZtGmTwX+SzmQMRLd2Quc8hdWBDMyFscoQv6o6NiJNkcEYf7/cLVCiZorOjFQB8cJZQqcDRSAAV/z76SxsLyKvnfYYyYdJHiIiO0MD8UjKS/0KgC+IyCD22jvDP/eS/DiA385l0AZEq7XH9rhdVXXbzcJ6TNmsZsoQv6o6NiJN3cK+Xw3825MjzLfcxkCA8f/is7DSiSdQZOHS8KdRlgtrK4Czw7/PBnBFyrFnYcJ9FRodMGgsdAaA2x2MUYnoQ3vM8aqqE9OFcI1NSmAZ4ldVxyZtNRJiir5fzib3WIqty0UQELtWfBA9lUkZ4DTKao9flgG5EMDJJO8BcHJ4GyS3kPyb6CCSmwEcDuDrE8//NMnbANwGYCOAPyxgzImM/bpuL/Ai02Nt0kXLEL+qOjaFc5EQ03iB4s69FATR3blCOgVdK7OCjg5QWe3xS9lDishjAF6TcP82AO+M3X4QwKEJx73a5fh0iC6CR5ddDG7e0k67gWcWC2plYrGaiX/p95tv5Tms2mJbed1pN2PfL4cu0rCZoss4HhBcKy6EsWYNHR2gtbYDmRkmdyCuK4WLwGY1o+O3XStEokA2cSXnLqxWA3uHS9i9122qMBBcKy6EsWYN1SD6sgBcCRLB3oBYMt8M0yx7AyPBIFWCIHpBMRCLnl5Fa5fUAVuRpm67gad6g+W/XRC97tOL7l1YT/UG3n2lQDS3ZF1LZTan9AbEkrk5Lq8sTQSDVAmCnNk9cfJg0aKYrOjGj3XAVqQp/jxXq8z45OMujteM/e0NSBbR3JIla2sqAJcH3oDkQHQxuExL7BaZhWWRU160+FUdsJUbLWJy7xQwucddeL6NiRoqsrZldjf2BiQHoova5QcY1VeotHa2JY8guo+BjLEOorfcT+4rjZSbhVBjjssuXr8DUUMlLd4bkJrTLcCAdNpNiGJrZ1usgugFi1/Vgaiw0tYoA24r0ZfP53B3ML5WfBGhCirJM5F8QhmFmd6A5ED0wbn0QRYVnBaxE8byMZDV2Io0Re+pS8GguNFwu5N2f63MEiqytn4HUnOii8/tDqSYlf3iYAkiNv56HwOZxFakqVPAqj3+2i4n9yLcvbNEEPtUMyAz1UxxLbEcRHeYh11UbGGhbyf2U7R6Yh2wFWkaf7/cT+zB+dx/j70BUSOIfaYvGm2TNGzwBiQHigqiA+4nZtvVzNjQ+RhIhG2biWhCL+L75fo80ffK5WJrltALovsYSC0ZXxTuLrx5DW0AG3qWAd9Wo1jxqzpgKzfaKcBFWkSqcPw8fgeiRkfJhWVXZ2SDNyA5MK4DcR987DmuRs8jIOc78q7EVqQpeq4rOdvJ1y7CVeaD6GqoNFHt+RhIvekU6GJw78IKVzMWLgYva7sSW62GIlbt65pziEI0bl1YYRaWLyRUotNuZu9ABiM054h2Cc0pvQHJgULqQIpyYeWwAylS/KoO2OpVj11Y7nzcJAtJsfUuLD267Qb6oyUMR9Prv8rqxAt4A5ILhbQyKSwLKwcXVoFtV+qArUhTUbUTnXYDjTmibSBlrEoR7t5ZYtnzkLIgc6nhkoU3IDlQTBZWMfUVeTRmK1L8qg7YXuBF1U502w2nDUGBYmpaZgkVeYSy5GyBkgwIyTeRvIPkEsktKcedQvJukveSPD92/1EkbyB5D8nLSbaLGXkyRWzL51uBtKnrlf2CZdU04IPokwQa8/YxEOc7kFbD+Tm8C0sPldhnzzLLz4aydiC3A/gPAL4x7QCSDQAfAXAqgOMAnEXyuPDhPwZwkYgcC+AJAO9wO9x0iggMkkFrZ+dB9By0BXwQfSULA1sXVjjpOq6d6LYbzif2Itr+zBLR3JLWgaIsOVugPEnbuwBkbZVPBHCviNwfHnsZgNNJ3gXg1QB+NTzuUgAfBPCXrsabRVEN4rrtBv7+pu34+g92OTvHY3v61sJY3XYT9z+6Gyf/6aSU/drkqd6gJi6sJnoDt806i2j7M0tE79O7PnXT1AXqQ08s4KWbNxQ5rGWq7Ig8FMBDsdvbAfwMgAMBPCkiw9j9q3TTI0ieA+AcADjiiCOcDPSlmzfgnFc+Fyccud7J60f8l1cdg20/fNzpOY4F8JMH72/lB3/TCYdh79DvQCJ+4tn74Q0vfo7x8zftuw7vee1P4JQXPjvHUa3mHf/uKOzZ69ZF+vPPOwj/+VVH45hN+zo9z6xw/BEH4I0nHJa6Azn24H1xxvFTp0Cn0JW+BMmvAkj6xr9XRK4Ij/kagN8WkW0Jz38TgNeLyDvD27+GYFfyIQDfBP9ivwAAB0NJREFUFpFjwvsPB3CViLwoa0xbtmyRbdtWncrj8Xg8KZC8SURWxaud7UBE5LWWL7EdwOGx24cB2AHgUQAHkGyGu5Dofo/H4/EUSJXTeG8EcGyYcdUGcCaArRJsma4D8MbwuLMBXFHSGD0ej2fNUlYa7y+R3A7g5QC+SPLL4f3PIXkVAIS7i3MBfBnAXQA+KyJ3hC9xHoDfInkvgpjIx4r+Hzwej2et4ywGUkV8DMTj8Xj0mRYDqbILy+PxeDwVxhsQj8fj8RjhDYjH4/F4jPAGxOPxeDxGrKkgOsldAH5o+PSNCGpQqk5dxgnUZ6x+nPlSl3EC9Rmr63EeKSKbJu9cUwbEBpLbkrIQqkZdxgnUZ6x+nPlSl3EC9RlrWeP0LiyPx+PxGOENiMfj8XiM8AZEnYvLHoAidRknUJ+x+nHmS13GCdRnrKWM08dAPB6Px2OE34F4PB6PxwhvQDwej8djhDcgCpA8heTdJO8leX7Z45kGyQdJ3kbyFpKV6RpJ8hKSj5C8PXbfBpJXk7wn/O1WzlGRKWP9IMl/C9/XW0j+QpljDMd0OMnrSN5F8g6S7w7vr9T7mjLOSr2nJOdJfofk98Jx/n54/1Ekbwjfz8tDaYkqjvMTJB+IvZ/HFzIeHwNJh2QDwA8AnIxA5OpGAGeJyJ2lDiwBkg8C2CIilSp8IvlKALsBfFJEXhje9ycAHheRC0OjvF5EzitznOG4ksb6QQC7ReR/lTm2OCQPAXCIiHyX5H4AbgJwBoC3oULva8o4fwUVek8ZaDjvIyK7SbYAfBPAuwH8FoB/EJHLSP4VgO+JyF9WcJy/DuBKEflckePxO5BsTgRwr4jcLyJ9AJcBOL3kMdUKEfkGgEkx99MBXBr+fSmCSaV0poy1cojIThH5bvj3Mwg0cw5Fxd7XlHFWCgnYHd5shT8C4NUAokm5Cu/ntHGWgjcg2RwK4KHY7e2o4AUQIgC+QvImkueUPZgMDhaRnUAwyQA4qOTxZHEuyVtDF1cl3G0RJDcDeAmAG1Dh93VinEDF3lOSDZK3AHgEwNUA7gPwZChuB1Tk2p8cp4hE7+eHw/fzIpLrihiLNyDZMOG+qvr9ThKRnwZwKoDfCN0xHnv+EsDRAI4HsBPA/y53OGNI7gvg8wD+m4g8XfZ4ppEwzsq9pyIyEpHjARyGwPPw/KTDih1VwgAmxknyhQAuAPA8AC8FsAGBaqtzvAHJZjuAw2O3DwOwo6SxpCIiO8LfjwD4AoKLoKo8HPrHIz/5IyWPZyoi8nB40S4B+GtU5H0NfeCfB/BpEfmH8O7Kva9J46zqewoAIvIkgK8BeBmAA0g2w4cqde3HxnlK6CoUEdkL4OMo6P30BiSbGwEcG2ZjtAGcCWBryWNaBcl9wiAlSO4D4HUAbk9/VqlsBXB2+PfZAK4ocSypRBNyyC+hAu9rGEz9GIC7RORPYw9V6n2dNs6qvackN5E8IPy7A+C1COI11wF4Y3hYFd7PpHF+P7ZoIII4TSHvp8/CUiBMMfwzAA0Al4jIh0se0ipIPhfBrgMAmgD+rirjJPkZAK9C0HL6YQAfAPCPAD4L4AgAPwLwJhEpPXg9ZayvQuBqEQAPAnhXFGcoC5KvAPAvAG4DsBTe/T8QxBcq876mjPMsVOg9JfliBEHyBoKF9WdF5EPhdXUZArfQzQDeEq7yqzbOawFsQuByvwXAr8eC7e7G4w2Ix+PxeEzwLiyPx+PxGOENiMfj8XiM8AbE4/F4PEZ4A+LxeDweI7wB8Xg8Ho8R3oB4PBaQHMU6oN7CjG7NJH+d5FtzOO+DJDfavo7HY4NP4/V4LCC5W0T2LeG8D6KCnZc9awu/A/F4HBDuEP441G74Dsljwvs/SPK3w7//K8k7wwZ4l4X3bSD5j+F914eFYyB5IMmvkLyZ5EcR69FG8i3hOW4h+dFQgsDjcY43IB6PHZ0JF9abY489LSInAvgLBJ0MJjkfwEtE5MUI9BwA4PcB3Bze9z8AfDK8/wMAvikiL0HQruQIACD5fABvRtBI83gAIwD/Md9/0eNJppl9iMfjSaEXTtxJfCb2+6KEx28F8GmS/4igtQsAvALALwOAiFwb7jyeBeCVAP5DeP8XST4RHv8aACcAuDFog4QOKtBA0bM28AbE43GHTPk74t8jMAynAXgfyRcgXT4g6TUI4FIRucBmoB6PCd6F5fG4482x39+OP0ByDsDhInIdgN8FcACAfQF8A6ELiuSrADwa6mfE7z8VQCTAdA2AN5I8KHxsA8kjHf5PHs8yfgfi8djRCdXhIv5ZRKJU3nUkb0CwUDtr4nkNAH8buqcI4CIReTLUX/84yVsBLGDcmv33AXyG5HcBfB1Bp12IyJ0kfw+BEuUcgAGA3wDww7z/UY9nEp/G6/E4wKfZetYC3oXl8Xg8HiP8DsTj8Xg8RvgdiMfj8XiM8AbE4/F4PEZ4A+LxeDweI7wB8Xg8Ho8R3oB4PB6Px4j/D2lle1RZfqidAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "episode_durations = []\n",
    "episode_rewards = []\n",
    "\n",
    "# implementation of algorithim\n",
    "for episode in range(num_episodes):\n",
    "    env.reset()\n",
    "#     state = env.get_state() #why need?\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    r = 0\n",
    "    \n",
    "    for timestep in count():\n",
    "        action = agent.select_action(state, policy_net) # is this expecting state as a tensor...\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action.item()) # item returns value of tensor, since action is a tensor, before had # obs, reward, done, _ = env.take_action(action) #this take_action is same as step, but just wrapps the reward in a tensor # reward = env.take_action(action)\n",
    "        # env.render()\n",
    "        \n",
    "        # fix rewards\n",
    "        if reward == 0:\n",
    "            reward = -1\n",
    "        elif reward == 0.5:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = 1\n",
    "        r += reward\n",
    "        \n",
    "        # yeah so right now reward is just going to be 0 or 1, and i should just do it at end of episode. like no need to keep storing it\n",
    "        # but then how will it train on a good move or not? can only evaluate at end...\n",
    "        # cause most of the rewards in the experience buffer are just 0, so not very helpful for training...\n",
    "        \n",
    "    \n",
    "    \n",
    "#        memory.push(Experience(state, action, next_state, reward_tensor))\n",
    "        memory.push(Experience(torch.tensor(state.board).float(), action, torch.tensor(next_state.board).float(), torch.tensor(reward, dtype=torch.float, device=mydevice))) # but why the [reward] and not just reward???\n",
    "        state = next_state\n",
    "        \n",
    "        \n",
    "        if memory.can_provide_sample(batch_size):\n",
    "            experiences = memory.sample(batch_size)\n",
    "            states, actions, rewards, next_states = extract_tensors(experiences)\n",
    "\n",
    "            current_q_values = QValues.get_current(policy_net, states, actions) # returns q values for any state action pairs as predicted by the policy net, returned as a pytorch tensor\n",
    "            \n",
    "            # for each next state, we want to obtain the maximum q value (predicted by target net) among all possible next actions\n",
    "            # This function will return the maximum q-values for the next states using using the best corresponding next actions\n",
    "            next_q_values = QValues.get_next(target_net, next_states)\n",
    "            target_q_values = (next_q_values * gamma) + rewards # calculating the target q-vals using the formula\n",
    "\n",
    "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1)) # calc loss using mean squared error\n",
    "            optimizer.zero_grad() # if we didn't zero out gradients, we would be accumulating gradients across all back prop runs.?????\n",
    "            loss.backward() # computes gradient of loss wrt all weights and biases in the policy net\n",
    "            optimizer.step() # updates the wrights and biases with the gradients that were computed when we called backward() \n",
    "            \n",
    "            \n",
    "        if done:\n",
    "            episode_durations.append(timestep)\n",
    "            episode_rewards.append(r)\n",
    "            plot(episode_rewards, 100)\n",
    "            \n",
    "            # ok so these episode_durations are just basically the reward it got... since its directly the episode duration. \n",
    "            # let see what the other guy is plotting\n",
    "            \n",
    "            break\n",
    "            \n",
    "            \n",
    "    # check to see if we need to update the target network (remember we set it to multiples of 10)\n",
    "    if episode % target_update == 0: \n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiences_test = memory.sample(50)\n",
    "print(experiences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_tensors(experiences):\n",
    "    \n",
    "#     # Convert batch of Experiences to Experience of batches, see below\n",
    "#     batch = Experience(*zip(*experiences))\n",
    "\n",
    "#     t1 = torch.cat(batch.state)\n",
    "#     t2 = torch.cat(batch.action)\n",
    "#     t3 = torch.cat(batch.reward)\n",
    "#     t4 = torch.cat(batch.next_state)\n",
    "\n",
    "#     return (t1,t2,t3,t4)\n",
    "\n",
    "# next line is current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "#     def get_current(policy_net, states, actions):\n",
    "#         return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1)) \n",
    "#     '''\n",
    "#     remember for each next state, we want to obtain the maximum q value we could get for\n",
    "#     that next state (across all the possible actions).\n",
    "#     For a DQN, that max q value will be predicted by the target net\n",
    "#     '''\n",
    "    \n",
    "\n",
    "\n",
    "# states, actions, rewards, next_states = extract_tensors(experiences)\n",
    "# current_q_values = QValues.get_current(policy_net, states, actions) # returns q values for any state action pairs as predicted by the policy net, returned as a pytorch tensor\n",
    "\n",
    "batch_test = Experience(*zip(*experiences_test))\n",
    "print(batch_test)\n",
    "\n",
    "# states_test1 = torch.cat(batch_test.state)\n",
    "# print(states_test1)\n",
    "\n",
    "states_test2 = torch.stack(batch_test.state)\n",
    "print(states_test2)\n",
    "\n",
    "actions_test = torch.stack(batch_test.action)\n",
    "print(actions_test)\n",
    "\n",
    "next_states_test = torch.stack(batch_test.next_state)\n",
    "print(next_states_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QValues.get_current(policy_net, states_test2, actions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = next_states_test.shape[0]\n",
    "values = torch.zeros(batch_size).to(mydevice) # initializing tensor and sending to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_net(next_states_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_net(next_states_test).max(dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# returns tensor with 0s for q-vals for final states, and target net's maximum predicted q-vals across all actions for each final state\n",
    "values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
    "return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qvals_test = torch.tensor([[1.0143],\n",
    "        [1.0678]])\n",
    "\n",
    "next_q_vals_test = torch.tensor([0.0631, 0.0623, 0.0639, 0.0626, 0.0624, 0.0627, 0.0624, 0.0625, 0.0623,\n",
    "        0.0623, 0.0623, 0.0623, 0.0627, 0.0633, 0.0623, 0.0621, 0.0622, 0.0625,\n",
    "        0.0624, 0.0623, 0.0628, 0.0000, 0.0625, 0.0620, 0.0622, 0.0622, 0.0624,\n",
    "        0.0623, 0.0623, 0.0621, 0.0623, 0.0623, 0.0000, 0.0623, 0.0622, 0.0626,\n",
    "        0.0628, 0.0623, 0.0000, 0.0626, 0.0624, 0.0622, 0.0623, 0.0626, 0.0626,\n",
    "        0.0624, 0.0624, 0.0625, 0.0000, 0.0624, 0.0624, 0.0633, 0.0626, 0.0622,\n",
    "        0.0623, 0.0624, 0.0624, 0.0627, 0.0627, 0.0623, 0.0623, 0.0623, 0.0631,\n",
    "        0.0624, 0.0627, 0.0633, 0.0623, 0.0625, 0.0624, 0.0623, 0.0625, 0.0626,\n",
    "        0.0623, 0.0624, 0.0623, 0.0626, 0.0628, 0.0622, 0.0623, 0.0624, 0.0625,\n",
    "        0.0623, 0.0626, 0.0623, 0.0623, 0.0623, 0.0625, 0.0624, 0.0626, 0.0625,\n",
    "        0.0631, 0.0628, 0.0619, 0.0623, 0.0624, 0.0623, 0.0624, 0.0623, 0.0623,\n",
    "        0.0632, 0.0626, 0.0623, 0.0000, 0.0629, 0.0623, 0.0629, 0.0624, 0.0623,\n",
    "        0.0624, 0.0623, 0.0624, 0.0624, 0.0624, 0.0625, 0.0625, 0.0623, 0.0623,\n",
    "        0.0624, 0.0628, 0.0623, 0.0000, 0.0626, 0.0623, 0.0622, 0.0623, 0.0623,\n",
    "        0.0625, 0.0626, 0.0624, 0.0623, 0.0633, 0.0624, 0.0623, 0.0620, 0.0625,\n",
    "        0.0623, 0.0624, 0.0626, 0.0643, 0.0624, 0.0623, 0.0624, 0.0623, 0.0630,\n",
    "        0.0625, 0.0626, 0.0624, 0.0625, 0.0624, 0.0622, 0.0623, 0.0624, 0.0625,\n",
    "        0.0628, 0.0634, 0.0623, 0.0622, 0.0624, 0.0618, 0.0625, 0.0625, 0.0622,\n",
    "        0.0623, 0.0627, 0.0622, 0.0622, 0.0624, 0.0643, 0.0623, 0.0623, 0.0626,\n",
    "        0.0624, 0.0631, 0.0624, 0.0625, 0.0627, 0.0625, 0.0625, 0.0625, 0.0624,\n",
    "        0.0624, 0.0000, 0.0625, 0.0621, 0.0623, 0.0625, 0.0624, 0.0626, 0.0623,\n",
    "        0.0624, 0.0623, 0.0624, 0.0000, 0.0623, 0.0624, 0.0618, 0.0623, 0.0639,\n",
    "        0.0623, 0.0626, 0.0000, 0.0621, 0.0639, 0.0624, 0.0623, 0.0623, 0.0623,\n",
    "        0.0630, 0.0645, 0.0624, 0.0627, 0.0622, 0.0621, 0.0633, 0.0629, 0.0000,\n",
    "        0.0623, 0.0623, 0.0626, 0.0626, 0.0627, 0.0624, 0.0625, 0.0625, 0.0628,\n",
    "        0.0630, 0.0624, 0.0621, 0.0622, 0.0623, 0.0629, 0.0625, 0.0626, 0.0623,\n",
    "        0.0624, 0.0623, 0.0624, 0.0000, 0.0631, 0.0624, 0.0622, 0.0623, 0.0624,\n",
    "        0.0636, 0.0624, 0.0631, 0.0623, 0.0623, 0.0625, 0.0624, 0.0623, 0.0627,\n",
    "        0.0623, 0.0624, 0.0000, 0.0624])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qvals_test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_q_vals_test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use deeplizard DQN class and stuff, see if can get working with a gym conenct4?\n",
    "\n",
    "# so with kaggles connectX class/env, I can easily train the agent against itself? doen't have to set that up?\n",
    "# i.e --> self.trainer = self.env.train(self.pair)\n",
    "\n",
    "# so do I need to import gym or nah...? can it easily train against itself?\n",
    "\n",
    "# so the ConnectX class inherits the gym.env class... ok\n",
    "\n",
    "# making it inherit the gym class ensures it will have the following methods:\n",
    "#     step\n",
    "#     reset\n",
    "#     render\n",
    "#     close\n",
    "#     seed\n",
    "# full implementations of kaggles Environment have implementations for step, reset, render, etc\n",
    "# see here: https://github.com/Kaggle/kaggle-environments/blob/master/kaggle_environments/core.py\n",
    "\n",
    "# ok..so what is kaggles env.make doing?\n",
    "\n",
    "# has its own lightweight train method for training, ex\n",
    "            # Training agent in first position (player 1) against the default random agent.\n",
    "            trainer = env.train([None, \"random\"])\n",
    "        \n",
    "        \n",
    "# ok so he's just modifying the connectX class, but why? i guess we will see\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ok so for now lets just see if i can successfully train something\n",
    "\n",
    "# lets try to implement it using deeplizards DQN, and see if can train using it. Then, compare with that dudes on stuff like how \n",
    "# his training algo is slightly different, his hyperparms, and like how fast his takes\n",
    "\n",
    "'''\n",
    "so I think the part I was worried most about is handled, like how to train my network against a different agent. I guess when\n",
    "we originally create the ConnectX() env we can just choose the trainer or something to be the negmax agent and train against that\n",
    "and then i guess when we can beat that we can train against our current best agent...\n",
    "\n",
    "\n",
    "ok so i realized that can't use:\n",
    "    from collections import namedtuple\n",
    "    from itertools import count\n",
    "    \n",
    "but I think i can re-organize it to do it. Also need to fix how i'm storing my observations/states? are they the same thing?\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is kaggles connect4 env\n",
    "\n",
    "\n",
    "\n",
    "from kaggle_environments import make\n",
    "import gym\n",
    "\n",
    "class ConnectX(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.env = make(\"connectx\", debug=True)\n",
    "        self.trainer = self.env.train([None, \"random\"])\n",
    "        \n",
    "        # Define required gym fields (examples):\n",
    "        config = self.env.configuration\n",
    "        self.action_space = gym.spaces.Discrete(config.columns)\n",
    "        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.trainer.step(action)\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.trainer.reset()\n",
    "    \n",
    "    def render(self, **kwargs):\n",
    "        return self.env.render(**kwargs)\n",
    "        \n",
    "    \n",
    "env = ConnectX()\n",
    "\n",
    "done = False\n",
    "obs = env.reset()\n",
    "while not done:\n",
    "    # Choose first available empty column as the action.\n",
    "    action = [i for i in range(len(obs.board)) if obs.board[i] == 0][0]\n",
    "    obs, reward, done, info = env.step(action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deeplizard, comments are what other guy has\n",
    "batch_size = 256 # 32\n",
    "gamma = 0.999 # 0.99\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.001\n",
    "target_update = 10 # guessing this is \"copy step\" = 25\n",
    "memory_size = 100000\n",
    "lr = 0.001 #.01\n",
    "num_episodes = 1000 # episodes = 16000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# additions from other guy\n",
    "copy_step = 25\n",
    "hidden_units = [100, 200, 200, 100]\n",
    "\n",
    "max_experiences = 10000\n",
    "min_experiences = 100\n",
    "\n",
    "# epsilon = 0.5        looks like these all correspond\n",
    "# decay = 0.9999\n",
    "# min_epsilon = 0.1\n",
    "\n",
    "\n",
    "precision = 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
